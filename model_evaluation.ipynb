{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data\n",
    "from common import OUTPUTPATH\n",
    "from modules.experiments import KFold, GroupKFoldSpecial\n",
    "STATE = np.random.RandomState(seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.201363</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>0.207675</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.571579</td>\n",
       "      <td>0.132058</td>\n",
       "      <td>2.245177</td>\n",
       "      <td>0.084801</td>\n",
       "      <td>2.348206</td>\n",
       "      <td>0.409243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263048</td>\n",
       "      <td>0.476165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.056754</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.177456</td>\n",
       "      <td>0.085135</td>\n",
       "      <td>0.229125</td>\n",
       "      <td>0.015097</td>\n",
       "      <td>0.910888</td>\n",
       "      <td>0.457557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.201363                0.008631   \n",
       "GAM                     0.036842                0.003132   \n",
       "RuleFit                 0.005737                0.001321   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.207675               0.047621          0.553817   \n",
       "GAM                    0.088377               0.033908          0.122612   \n",
       "RuleFit                0.056754               0.029144          0.018056   \n",
       "RF                     0.065461               0.026606          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.038090         0.571579        0.132058   \n",
       "GAM             0.010827         0.270439        0.105647   \n",
       "RuleFit         0.004359         0.177456        0.085135   \n",
       "RF              0.000319         0.199298        0.067024   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.245177            0.084801            2.348206   \n",
       "GAM                 0.647140            0.038226            1.263048   \n",
       "RuleFit             0.229125            0.015097            0.910888   \n",
       "RF                  0.243237            0.003150            1.069053   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.409243  \n",
       "GAM               0.476165  \n",
       "RuleFit           0.457557  \n",
       "RF                0.685669  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'interpolation.csv'))\n",
    "interpolation = interpolation.set_index('Unnamed: 0')\n",
    "interpolation.index.name = None\n",
    "interpolation.index = ['LR', 'GAM', 'RuleFit', 'RF']\n",
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.204002</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.199708</td>\n",
       "      <td>0.166414</td>\n",
       "      <td>0.566550</td>\n",
       "      <td>0.037625</td>\n",
       "      <td>0.608891</td>\n",
       "      <td>0.447396</td>\n",
       "      <td>2.273170</td>\n",
       "      <td>0.083029</td>\n",
       "      <td>2.674861</td>\n",
       "      <td>1.793803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>0.420958</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>2.544994</td>\n",
       "      <td>3.358591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.131695</td>\n",
       "      <td>0.175832</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.302883</td>\n",
       "      <td>0.392765</td>\n",
       "      <td>0.224562</td>\n",
       "      <td>0.019692</td>\n",
       "      <td>2.153337</td>\n",
       "      <td>2.820715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.118843</td>\n",
       "      <td>0.184329</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.238704</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.640178</td>\n",
       "      <td>1.858515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.204002                0.010628   \n",
       "GAM                     0.040066                0.003470   \n",
       "RuleFit                 0.005270                0.001165   \n",
       "RF                      0.000294                0.000472   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.199708               0.166414          0.566550   \n",
       "GAM                    0.146702               0.173823          0.133519   \n",
       "RuleFit                0.131695               0.175832          0.016251   \n",
       "RF                     0.118843               0.184329          0.000588   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.037625         0.608891        0.447396   \n",
       "GAM             0.010856         0.376702        0.420958   \n",
       "RuleFit         0.003606         0.302883        0.392765   \n",
       "RF              0.000943         0.264250        0.388123   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.273170            0.083029            2.674861   \n",
       "GAM                 0.671179            0.051213            2.544994   \n",
       "RuleFit             0.224562            0.019692            2.153337   \n",
       "RF                  0.238704            0.003475            1.640178   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                1.793803  \n",
       "GAM               3.358591  \n",
       "RuleFit           2.820715  \n",
       "RF                1.858515  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'extrapolation.csv'))\n",
    "extrapolation = extrapolation.set_index('Unnamed: 0')\n",
    "extrapolation.index.name = None\n",
    "extrapolation.index = ['LR', 'GAM', 'RuleFit', 'RF']\n",
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_summary(metric, summ, num_reps=30, baseline=None, names=None, colors = list(mcolors.BASE_COLORS.keys())):\n",
    "    width = 0.35\n",
    "    ind = np.arange(len(summ))\n",
    "    plt.bar(ind-width/2, summ[f'mean_train_{metric}'], width=width, label='train', \n",
    "            yerr=summ[f'std_train_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    plt.bar(ind+width/2, summ[f'mean_test_{metric}'], width=width, label='test',\n",
    "            yerr=summ[f'std_test_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    if baseline:\n",
    "        for i in range(len(baseline)):\n",
    "            plt.axhline(y=baseline[i], color=colors[i], linestyle='-', label=names[i])\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(ind, summ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_loss(_train, _test):\n",
    "    \"\"\"This is calculate realistic full phase error\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if (test_uniq[i] == _select).all():\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)\n",
    "\n",
    "def get_hamming_loss(_train, _test, name):\n",
    "    \"\"\"This is calculate realistic hamming loss\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if test_uniq[i] == _select:\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full sphase error base line:\n",
      "Unreal inform error:0.5912162162162162 \n",
      "Uninform error:0.9375\n"
     ]
    }
   ],
   "source": [
    "# interpolation full phase\n",
    "uniq, cnts = np.unique(data.y.values, return_counts=True, axis=0)\n",
    "unreal_inter_full_info_error = 1 - max(cnts)/sum(cnts)\n",
    "inter_full_uninfo_error = 1 - (1/2)**4\n",
    "print('Interpolation full sphase error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_full_info_error, inter_full_uninfo_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.27956081081081074 \n",
      "Uninform error:0.5\n"
     ]
    }
   ],
   "source": [
    "# interpolation hamming loss\n",
    "unreal_inter_hamming_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    error = 1 - max(cnt)/sum(cnt)\n",
    "    unreal_inter_hamming_loss.append(error)\n",
    "unreal_inter_hamming_loss = np.mean(unreal_inter_hamming_loss)\n",
    "inter_uninfo_hamming_loss = np.mean([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_hamming_loss, inter_uninfo_hamming_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Unreal inform error:0.5233630952380952\n"
     ]
    }
   ],
   "source": [
    "# extrapolation full phase loss\n",
    "unreal_extra_full_info_error = []\n",
    "cnt =0\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    unreal_extra_full_info_error.append(get_full_loss(train, test))\n",
    "unreal_extra_full_info_error = np.mean(unreal_extra_full_info_error)\n",
    "unreal_extra_full_info_error\n",
    "print('Extrapolation full sphase error base line:\\nUnreal inform error:{}'.format(unreal_extra_full_info_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2485863095238095\n"
     ]
    }
   ],
   "source": [
    "# extrapolation hamming loss\n",
    "unreal_extra_hamming_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "        unreal_extra_hamming_loss.append(get_hamming_loss(data.y.index.tolist(), test, each.name))\n",
    "unreal_extra_hamming_loss = np.mean(unreal_extra_hamming_loss)\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{}'.format(unreal_extra_hamming_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def neglikehood(p, y):\n",
    "    result = (-math.log2(p)*y - (1-y)*math.log2(1-p))\n",
    "    return (result)\n",
    "\n",
    "def get_full_logs(_train, _test):\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = [list(each) for each in train_uniq]\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = [list(each) for each in test_uniq]\n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)\n",
    "\n",
    "def get_hamming_logs(_train, _test, name):\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = train_uniq.tolist()\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = test_uniq.tolist()\n",
    "    \n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full phase logloss base line:\n",
      " Unreal Inform error:  2.5225679165070907\n"
     ]
    }
   ],
   "source": [
    "# unrealistic interpolation\n",
    "uniq, cnt = np.unique(data.y.values, axis=0, return_counts=True)\n",
    "prob = cnt/sum(cnt)\n",
    "\n",
    "unreal_inter_full_info_loss = 0\n",
    "for each in cnt:\n",
    "    unreal_inter_full_info_loss += -each * math.log2(each/sum(cnt))\n",
    "unreal_inter_full_info_loss = unreal_inter_full_info_loss/sum(cnt)\n",
    "print(\"Interpolation full phase logloss base line:\\n Unreal Inform error: \", unreal_inter_full_info_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average morphology logloss base line:\n",
      "Unreal Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "# interpolation average sphase informed\n",
    "unreal_inter_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_inter_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_inter_avg_info_loss = np.mean(unreal_inter_avg_info_loss)\n",
    "print('Interpolation average morphology logloss base line:\\nUnreal Inform logloss:{}'.format(unreal_inter_avg_info_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Real Inform logloss:2.6554775150876018 \n",
      "Uninform logloss:4.0\n"
     ]
    }
   ],
   "source": [
    "real_extra_full_info_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    real_extra_full_info_loss.append(get_full_logs(train, test))\n",
    "real_extra_full_info_loss = np.mean(real_extra_full_info_loss)\n",
    "\n",
    "extra_full_uninfo_loss = -math.log2(0.5**4)\n",
    "\n",
    "print('Extrapolation full sphase error base line:\\nReal Inform logloss:{} \\nUninform logloss:{}'.format(real_extra_full_info_loss, extra_full_uninfo_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation average sphase error base line:\n",
      "Real Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "unreal_extra_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_extra_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_extra_avg_info_loss = np.mean(unreal_extra_avg_info_loss)\n",
    "print('Extrapolation average sphase error base line:\\nReal Inform logloss:{}'.format(unreal_extra_avg_info_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.201363</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>0.207675</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>0.553817</td>\n",
       "      <td>0.038090</td>\n",
       "      <td>0.571579</td>\n",
       "      <td>0.132058</td>\n",
       "      <td>2.245177</td>\n",
       "      <td>0.084801</td>\n",
       "      <td>2.348206</td>\n",
       "      <td>0.409243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263048</td>\n",
       "      <td>0.476165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.056754</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.177456</td>\n",
       "      <td>0.085135</td>\n",
       "      <td>0.229125</td>\n",
       "      <td>0.015097</td>\n",
       "      <td>0.910888</td>\n",
       "      <td>0.457557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.201363                0.008631   \n",
       "GAM                     0.036842                0.003132   \n",
       "RuleFit                 0.005737                0.001321   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.207675               0.047621          0.553817   \n",
       "GAM                    0.088377               0.033908          0.122612   \n",
       "RuleFit                0.056754               0.029144          0.018056   \n",
       "RF                     0.065461               0.026606          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.038090         0.571579        0.132058   \n",
       "GAM             0.010827         0.270439        0.105647   \n",
       "RuleFit         0.004359         0.177456        0.085135   \n",
       "RF              0.000319         0.199298        0.067024   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.245177            0.084801            2.348206   \n",
       "GAM                 0.647140            0.038226            1.263048   \n",
       "RuleFit             0.229125            0.015097            0.910888   \n",
       "RF                  0.243237            0.003150            1.069053   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.409243  \n",
       "GAM               0.476165  \n",
       "RuleFit           0.457557  \n",
       "RF                0.685669  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.204002</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.199708</td>\n",
       "      <td>0.166414</td>\n",
       "      <td>0.566550</td>\n",
       "      <td>0.037625</td>\n",
       "      <td>0.608891</td>\n",
       "      <td>0.447396</td>\n",
       "      <td>2.273170</td>\n",
       "      <td>0.083029</td>\n",
       "      <td>2.674861</td>\n",
       "      <td>1.793803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>0.420958</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>2.544994</td>\n",
       "      <td>3.358591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.131695</td>\n",
       "      <td>0.175832</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.302883</td>\n",
       "      <td>0.392765</td>\n",
       "      <td>0.224562</td>\n",
       "      <td>0.019692</td>\n",
       "      <td>2.153337</td>\n",
       "      <td>2.820715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.118843</td>\n",
       "      <td>0.184329</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.238704</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.640178</td>\n",
       "      <td>1.858515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.204002                0.010628   \n",
       "GAM                     0.040066                0.003470   \n",
       "RuleFit                 0.005270                0.001165   \n",
       "RF                      0.000294                0.000472   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.199708               0.166414          0.566550   \n",
       "GAM                    0.146702               0.173823          0.133519   \n",
       "RuleFit                0.131695               0.175832          0.016251   \n",
       "RF                     0.118843               0.184329          0.000588   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.037625         0.608891        0.447396   \n",
       "GAM             0.010856         0.376702        0.420958   \n",
       "RuleFit         0.003606         0.302883        0.392765   \n",
       "RF              0.000943         0.264250        0.388123   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.273170            0.083029            2.674861   \n",
       "GAM                 0.671179            0.051213            2.544994   \n",
       "RuleFit             0.224562            0.019692            2.153337   \n",
       "RF                  0.238704            0.003475            1.640178   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                1.793803  \n",
       "GAM               3.358591  \n",
       "RuleFit           2.820715  \n",
       "RF                1.858515  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'c', 'm', 'y', 'b']\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, sharex=True, figsize=(20, 5))\n",
    "\n",
    "width = 0.25\n",
    "fontsize = 10\n",
    "ind = np.arange(len(interpolation))\n",
    "\n",
    "ax1.bar(ind-width, interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind, interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind+width, extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax1.set_title('Individual morphology error rate', fontsize=fontsize)\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax1.set_xticklabels(interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax1.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "ax1.sharey(ax2) ########### here to share the y axis\n",
    "\n",
    "ax2.bar(ind-width, interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind, interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind+width, extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax2.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax2.set_title('Full phase error rate', fontsize=fontsize)\n",
    "ax2.set_xticks(ind)\n",
    "ax2.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax2.sharey(ax1) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind, interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind+width, extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax3.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax3.set_title('Log loss', fontsize=fontsize)\n",
    "ax3.set_xticks(ind)\n",
    "ax3.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax3.set_ylabel('logloss', fontsize=fontsize)\n",
    "plt.savefig(os.path.join(OUTPUTPATH, 'latest_overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0   0.241  0.414   0.345    0.0   22.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2   0.938    0.0     0.0  0.062   16.0\n",
       "3     0.5  0.312   0.188    0.0   15.0\n",
       "4   0.857    0.0     0.0  0.143   14.0\n",
       "5     1.0    0.0     0.0    0.0   11.0\n",
       "6     1.0    0.0     0.0    0.0    8.0\n",
       "7   0.875    0.0     0.0  0.125    8.0\n",
       "8     1.0    0.0     0.0    0.0    7.0\n",
       "9   0.222  0.556   0.222    0.0    7.0\n",
       "10    nan    nan     nan    nan    7.0\n",
       "11    1.0    0.0     0.0    0.0    7.0\n",
       "12    1.0    0.0     0.0    0.0    6.0\n",
       "13  0.667    0.0     0.0  0.333    6.0\n",
       "14    1.0    0.0     0.0    0.0    6.0\n",
       "15    1.0    0.0     0.0    0.0    6.0\n",
       "16  0.167  0.667   0.167    0.0    5.0\n",
       "17    0.5  0.333   0.167    0.0    3.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    1.0    0.0     0.0    0.0    2.0\n",
       "20    0.0    0.0     0.0    1.0    2.0\n",
       "21    0.0    0.0     1.0    0.0    1.0\n",
       "22    0.0    0.0     1.0    0.0    1.0\n",
       "23    0.0    0.0     0.0    1.0    1.0\n",
       "24    0.0    1.0     0.0    0.0    1.0\n",
       "25    0.0    1.0     0.0    0.0    1.0\n",
       "26    0.0    1.0     0.0    0.0    1.0\n",
       "27    0.0    1.0     0.0    0.0    1.0\n",
       "28    0.0    1.0     0.0    0.0    1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrapolation hold out distribution\n",
    "\n",
    "from modules.experiments import GroupKFoldSpecial\n",
    "import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sphere':[], 'worm': [], 'vesicle': [], 'other':[], 'counts':[]})\n",
    "kf = GroupKFoldSpecial(len(set(data.comp_ids)), size=22)\n",
    "for train_indx, test_indx in kf.split(data.x1, data.y.replace(-1, 0), groups=data.comp_ids.array):\n",
    "    temp = pd.DataFrame(data.y.iloc[test_indx,].sum()/sum(data.y.iloc[test_indx,].sum())).T\n",
    "    temp['counts'] = int(len(test_indx))\n",
    "    df = pd.concat([df, temp], ignore_index=True)\n",
    "df = df.round(3)\n",
    "df = df.astype(str)\n",
    "df\n",
    "# df.sphere = df.apply(lambda x: round(x.sphere) if x.sphere == 0 or x.sphere == 1 else x.sphere, axis=1)\n",
    "# df.loc[1, 'sphere'] = 0.99\n",
    "# df\n",
    "# df.round(3)#(0.000000, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0   0.241  0.414   0.345    0.0   22.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2   0.938    0.0     0.0  0.062   16.0\n",
       "3     0.5  0.312   0.188    0.0   15.0\n",
       "4   0.857    0.0     0.0  0.143   14.0\n",
       "5     1.0    0.0     0.0    0.0   11.0\n",
       "6     1.0    0.0     0.0    0.0    8.0\n",
       "7   0.875    0.0     0.0  0.125    8.0\n",
       "8     1.0    0.0     0.0    0.0    7.0\n",
       "9   0.222  0.556   0.222    0.0    7.0\n",
       "10    nan    nan     nan    nan    7.0\n",
       "11    1.0    0.0     0.0    0.0    7.0\n",
       "12    1.0    0.0     0.0    0.0    6.0\n",
       "13  0.667    0.0     0.0  0.333    6.0\n",
       "14    1.0    0.0     0.0    0.0    6.0\n",
       "15    1.0    0.0     0.0    0.0    6.0\n",
       "16  0.167  0.667   0.167    0.0    5.0\n",
       "17    0.5  0.333   0.167    0.0    3.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    1.0    0.0     0.0    0.0    2.0\n",
       "20    0.0    0.0     0.0    1.0    2.0\n",
       "21    0.0    0.0     1.0    0.0    1.0\n",
       "22    0.0    0.0     1.0    0.0    1.0\n",
       "23    0.0    0.0     0.0    1.0    1.0\n",
       "24    0.0    1.0     0.0    0.0    1.0\n",
       "25    0.0    1.0     0.0    0.0    1.0\n",
       "26    0.0    1.0     0.0    0.0    1.0\n",
       "27    0.0    1.0     0.0    0.0    1.0\n",
       "28    0.0    1.0     0.0    0.0    1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrapolation hold out distribution\n",
    "\n",
    "from modules.experiments import GroupKFoldSpecial\n",
    "import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sphere':[], 'worm': [], 'vesicle': [], 'other':[], 'counts':[]})\n",
    "kf = GroupKFoldSpecial(len(set(data.comp_ids)), size=22)\n",
    "for train_indx, test_indx in kf.split(data.x1, data.y.replace(-1, 0), groups=data.comp_ids.array):\n",
    "    temp = pd.DataFrame(data.y.iloc[test_indx,].sum()/sum(data.y.iloc[test_indx,].sum())).T\n",
    "    temp['counts'] = int(len(test_indx))\n",
    "    df = pd.concat([df, temp], ignore_index=True)\n",
    "df = df.round(3)\n",
    "df = df.astype(str)\n",
    "df\n",
    "# df.sphere = df.apply(lambda x: round(x.sphere) if x.sphere == 0 or x.sphere == 1 else x.sphere, axis=1)\n",
    "# df.loc[1, 'sphere'] = 0.99\n",
    "# df\n",
    "# df.round(3)#(0.000000, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0     nan    nan     nan    nan    7.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2     1.0    0.0     0.0    0.0   11.0\n",
       "3     1.0    0.0     0.0    0.0    8.0\n",
       "4     1.0    0.0     0.0    0.0    7.0\n",
       "5     1.0    0.0     0.0    0.0    7.0\n",
       "6     1.0    0.0     0.0    0.0    6.0\n",
       "7     1.0    0.0     0.0    0.0    6.0\n",
       "8     1.0    0.0     0.0    0.0    6.0\n",
       "9     1.0    0.0     0.0    0.0    2.0\n",
       "10    0.0    1.0     0.0    0.0    1.0\n",
       "11    0.0    1.0     0.0    0.0    1.0\n",
       "12    0.0    1.0     0.0    0.0    1.0\n",
       "13    0.0    1.0     0.0    0.0    1.0\n",
       "14    0.0    1.0     0.0    0.0    1.0\n",
       "15    0.0    0.0     1.0    0.0    1.0\n",
       "16    0.0    0.0     1.0    0.0    1.0\n",
       "17    0.0    0.0     0.0    1.0    1.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    0.0    0.0     0.0    1.0    2.0\n",
       "20  0.241  0.414   0.345    0.0   22.0\n",
       "21  0.938    0.0     0.0  0.062   16.0\n",
       "22    0.5  0.312   0.188    0.0   15.0\n",
       "23  0.857    0.0     0.0  0.143   14.0\n",
       "24  0.875    0.0     0.0  0.125    8.0\n",
       "25  0.222  0.556   0.222    0.0    7.0\n",
       "26  0.667    0.0     0.0  0.333    6.0\n",
       "27  0.167  0.667   0.167    0.0    5.0\n",
       "28    0.5  0.333   0.167    0.0    3.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reindex([10, 1, 5, 6, 8, 11, 12, 14, 15, 19, 24, 25, 26, 27, 28, 21, 22, 23, 18, 20, 0, 2, 3, 4, 7, 9, 13, 16, 17]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corona_GMA, core_HEMA has no morphology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1304dd6425b372aa8285a68c11c3f25cfbbc0c3931e5f36be2c30c57a28c8b2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
