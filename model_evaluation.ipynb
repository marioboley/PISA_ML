{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data\n",
    "from common import OUTPUTPATH\n",
    "from modules.experiments import KFold, GroupKFoldSpecial\n",
    "STATE = np.random.RandomState(seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>2.359290</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263058</td>\n",
       "      <td>0.476161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.207544</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.323129</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>1.047064</td>\n",
       "      <td>0.429802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.202688                0.010509   \n",
       "GAM                     0.036842                0.003132   \n",
       "RuleFit                 0.013761                0.002088   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.215175               0.055519          0.556380   \n",
       "GAM                    0.088377               0.033908          0.122612   \n",
       "RuleFit                0.066754               0.033378          0.044968   \n",
       "RF                     0.065461               0.026606          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.043451         0.588246        0.146107   \n",
       "GAM             0.010827         0.270439        0.105647   \n",
       "RuleFit         0.005734         0.207544        0.090273   \n",
       "RF              0.000319         0.199298        0.067024   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.251545            0.100844            2.359290   \n",
       "GAM                 0.647140            0.038226            1.263058   \n",
       "RuleFit             0.323129            0.010737            1.047064   \n",
       "RF                  0.243237            0.003150            1.069053   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.414337  \n",
       "GAM               0.476161  \n",
       "RuleFit           0.429802  \n",
       "RF                0.685669  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'interpolation.csv'))\n",
    "interpolation = interpolation.set_index('Unnamed: 0')\n",
    "interpolation.index.name = None\n",
    "interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lr_pcc</th>\n",
       "      <td>0.203088</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>0.208413</td>\n",
       "      <td>0.167173</td>\n",
       "      <td>0.562837</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>0.624005</td>\n",
       "      <td>0.437829</td>\n",
       "      <td>2.274070</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>2.761258</td>\n",
       "      <td>2.031499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM_pcc</th>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>0.420958</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>2.545130</td>\n",
       "      <td>3.358488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit_pcc</th>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.184799</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.337756</td>\n",
       "      <td>0.419485</td>\n",
       "      <td>0.273653</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>2.351826</td>\n",
       "      <td>3.136170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_pcc</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.118843</td>\n",
       "      <td>0.184329</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.238704</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.640178</td>\n",
       "      <td>1.858515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_train_hamming loss  std_train_hamming loss  \\\n",
       "Lr_pcc                    0.203088                0.011308   \n",
       "GAM_pcc                   0.040066                0.003470   \n",
       "RuFit_pcc                 0.011257                0.002541   \n",
       "RF_pcc                    0.000294                0.000472   \n",
       "\n",
       "           mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "Lr_pcc                   0.208413               0.167173          0.562837   \n",
       "GAM_pcc                  0.146702               0.173823          0.133519   \n",
       "RuFit_pcc                0.141384               0.184799          0.037851   \n",
       "RF_pcc                   0.118843               0.184329          0.000588   \n",
       "\n",
       "           std_train_error  mean_test_error  std_test_error  \\\n",
       "Lr_pcc            0.039831         0.624005        0.437829   \n",
       "GAM_pcc           0.010856         0.376702        0.420958   \n",
       "RuFit_pcc         0.007831         0.337756        0.419485   \n",
       "RF_pcc            0.000943         0.264250        0.388123   \n",
       "\n",
       "           mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "Lr_pcc                2.274070            0.098860            2.761258   \n",
       "GAM_pcc               0.671179            0.051213            2.545130   \n",
       "RuFit_pcc             0.273653            0.030820            2.351826   \n",
       "RF_pcc                0.238704            0.003475            1.640178   \n",
       "\n",
       "           std_test_log loss  \n",
       "Lr_pcc              2.031499  \n",
       "GAM_pcc             3.358488  \n",
       "RuFit_pcc           3.136170  \n",
       "RF_pcc              1.858515  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'extrapolation.csv'))\n",
    "extrapolation = extrapolation.set_index('Unnamed: 0')\n",
    "extrapolation.index.name = None\n",
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_summary(metric, summ, num_reps=30, baseline=None, names=None, colors = list(mcolors.BASE_COLORS.keys())):\n",
    "    width = 0.35\n",
    "    ind = np.arange(len(summ))\n",
    "    plt.bar(ind-width/2, summ[f'mean_train_{metric}'], width=width, label='train', \n",
    "            yerr=summ[f'std_train_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    plt.bar(ind+width/2, summ[f'mean_test_{metric}'], width=width, label='test',\n",
    "            yerr=summ[f'std_test_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    if baseline:\n",
    "        for i in range(len(baseline)):\n",
    "            plt.axhline(y=baseline[i], color=colors[i], linestyle='-', label=names[i])\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(ind, summ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_loss(_train, _test):\n",
    "    \"\"\"This is calculate realistic full phase error\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if (test_uniq[i] == _select).all():\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)\n",
    "\n",
    "def get_hamming_loss(_train, _test, name):\n",
    "    \"\"\"This is calculate realistic hamming loss\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if test_uniq[i] == _select:\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full sphase error base line:\n",
      "Unreal inform error:0.5912162162162162 \n",
      "Uninform error:0.9375\n"
     ]
    }
   ],
   "source": [
    "# interpolation full phase\n",
    "uniq, cnts = np.unique(data.y.values, return_counts=True, axis=0)\n",
    "unreal_inter_full_info_error = 1 - max(cnts)/sum(cnts)\n",
    "inter_full_uninfo_error = 1 - (1/2)**4\n",
    "print('Interpolation full sphase error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_full_info_error, inter_full_uninfo_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.27956081081081074 \n",
      "Uninform error:0.5\n"
     ]
    }
   ],
   "source": [
    "# interpolation hamming loss\n",
    "unreal_inter_hamming_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    error = 1 - max(cnt)/sum(cnt)\n",
    "    unreal_inter_hamming_loss.append(error)\n",
    "unreal_inter_hamming_loss = np.mean(unreal_inter_hamming_loss)\n",
    "inter_uninfo_hamming_loss = np.mean([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_hamming_loss, inter_uninfo_hamming_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Unreal inform error:0.5233630952380952\n"
     ]
    }
   ],
   "source": [
    "# extrapolation full phase loss\n",
    "unreal_extra_full_info_error = []\n",
    "cnt =0\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    unreal_extra_full_info_error.append(get_full_loss(train, test))\n",
    "unreal_extra_full_info_error = np.mean(unreal_extra_full_info_error)\n",
    "unreal_extra_full_info_error\n",
    "print('Extrapolation full sphase error base line:\\nUnreal inform error:{}'.format(unreal_extra_full_info_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2485863095238095\n"
     ]
    }
   ],
   "source": [
    "# extrapolation hamming loss\n",
    "unreal_extra_hamming_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "        unreal_extra_hamming_loss.append(get_hamming_loss(data.y.index.tolist(), test, each.name))\n",
    "unreal_extra_hamming_loss = np.mean(unreal_extra_hamming_loss)\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{}'.format(unreal_extra_hamming_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def neglikehood(p, y):\n",
    "    result = (-math.log2(p)*y - (1-y)*math.log2(1-p))\n",
    "    return (result)\n",
    "\n",
    "def get_full_logs(_train, _test):\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = [list(each) for each in train_uniq]\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = [list(each) for each in test_uniq]\n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)\n",
    "\n",
    "def get_hamming_logs(_train, _test, name):\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = train_uniq.tolist()\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = test_uniq.tolist()\n",
    "    \n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full phase logloss base line:\n",
      " Unreal Inform error:  2.5225679165070907\n"
     ]
    }
   ],
   "source": [
    "# unrealistic interpolation\n",
    "uniq, cnt = np.unique(data.y.values, axis=0, return_counts=True)\n",
    "prob = cnt/sum(cnt)\n",
    "\n",
    "unreal_inter_full_info_loss = 0\n",
    "for each in cnt:\n",
    "    unreal_inter_full_info_loss += -each * math.log2(each/sum(cnt))\n",
    "unreal_inter_full_info_loss = unreal_inter_full_info_loss/sum(cnt)\n",
    "print(\"Interpolation full phase logloss base line:\\n Unreal Inform error: \", unreal_inter_full_info_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average morphology logloss base line:\n",
      "Unreal Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "# interpolation average sphase informed\n",
    "unreal_inter_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_inter_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_inter_avg_info_loss = np.mean(unreal_inter_avg_info_loss)\n",
    "print('Interpolation average morphology logloss base line:\\nUnreal Inform logloss:{}'.format(unreal_inter_avg_info_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Real Inform logloss:2.6554775150876018 \n",
      "Uninform logloss:4.0\n"
     ]
    }
   ],
   "source": [
    "real_extra_full_info_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    real_extra_full_info_loss.append(get_full_logs(train, test))\n",
    "real_extra_full_info_loss = np.mean(real_extra_full_info_loss)\n",
    "\n",
    "extra_full_uninfo_loss = -math.log2(0.5**4)\n",
    "\n",
    "print('Extrapolation full sphase error base line:\\nReal Inform logloss:{} \\nUninform logloss:{}'.format(real_extra_full_info_loss, extra_full_uninfo_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation average sphase error base line:\n",
      "Real Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "unreal_extra_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_extra_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_extra_avg_info_loss = np.mean(unreal_extra_avg_info_loss)\n",
    "print('Extrapolation average sphase error base line:\\nReal Inform logloss:{}'.format(unreal_extra_avg_info_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>2.359290</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263058</td>\n",
       "      <td>0.476161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.207544</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.323129</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>1.047064</td>\n",
       "      <td>0.429802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.202688                0.010509   \n",
       "GAM                     0.036842                0.003132   \n",
       "RuleFit                 0.013761                0.002088   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.215175               0.055519          0.556380   \n",
       "GAM                    0.088377               0.033908          0.122612   \n",
       "RuleFit                0.066754               0.033378          0.044968   \n",
       "RF                     0.065461               0.026606          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.043451         0.588246        0.146107   \n",
       "GAM             0.010827         0.270439        0.105647   \n",
       "RuleFit         0.005734         0.207544        0.090273   \n",
       "RF              0.000319         0.199298        0.067024   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.251545            0.100844            2.359290   \n",
       "GAM                 0.647140            0.038226            1.263058   \n",
       "RuleFit             0.323129            0.010737            1.047064   \n",
       "RF                  0.243237            0.003150            1.069053   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.414337  \n",
       "GAM               0.476161  \n",
       "RuleFit           0.429802  \n",
       "RF                0.685669  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lr_pcc</th>\n",
       "      <td>0.203088</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>0.208413</td>\n",
       "      <td>0.167173</td>\n",
       "      <td>0.562837</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>0.624005</td>\n",
       "      <td>0.437829</td>\n",
       "      <td>2.274070</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>2.761258</td>\n",
       "      <td>2.031499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM_pcc</th>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>0.420958</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>2.545130</td>\n",
       "      <td>3.358488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit_pcc</th>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.184799</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.337756</td>\n",
       "      <td>0.419485</td>\n",
       "      <td>0.273653</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>2.351826</td>\n",
       "      <td>3.136170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_pcc</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.118843</td>\n",
       "      <td>0.184329</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.238704</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.640178</td>\n",
       "      <td>1.858515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_train_hamming loss  std_train_hamming loss  \\\n",
       "Lr_pcc                    0.203088                0.011308   \n",
       "GAM_pcc                   0.040066                0.003470   \n",
       "RuFit_pcc                 0.011257                0.002541   \n",
       "RF_pcc                    0.000294                0.000472   \n",
       "\n",
       "           mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "Lr_pcc                   0.208413               0.167173          0.562837   \n",
       "GAM_pcc                  0.146702               0.173823          0.133519   \n",
       "RuFit_pcc                0.141384               0.184799          0.037851   \n",
       "RF_pcc                   0.118843               0.184329          0.000588   \n",
       "\n",
       "           std_train_error  mean_test_error  std_test_error  \\\n",
       "Lr_pcc            0.039831         0.624005        0.437829   \n",
       "GAM_pcc           0.010856         0.376702        0.420958   \n",
       "RuFit_pcc         0.007831         0.337756        0.419485   \n",
       "RF_pcc            0.000943         0.264250        0.388123   \n",
       "\n",
       "           mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "Lr_pcc                2.274070            0.098860            2.761258   \n",
       "GAM_pcc               0.671179            0.051213            2.545130   \n",
       "RuFit_pcc             0.273653            0.030820            2.351826   \n",
       "RF_pcc                0.238704            0.003475            1.640178   \n",
       "\n",
       "           std_test_log loss  \n",
       "Lr_pcc              2.031499  \n",
       "GAM_pcc             3.358488  \n",
       "RuFit_pcc           3.136170  \n",
       "RF_pcc              1.858515  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'c', 'm', 'y', 'b']\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, sharex=True, figsize=(20, 5))\n",
    "\n",
    "width = 0.25\n",
    "fontsize = 10\n",
    "ind = np.arange(len(interpolation))\n",
    "\n",
    "ax1.bar(ind-width, interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind, interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind+width, extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax1.set_title('Individual morphology error rate', fontsize=fontsize)\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax1.set_xticklabels(interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax1.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "ax1.sharey(ax2) ########### here to share the y axis\n",
    "\n",
    "ax2.bar(ind-width, interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind, interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind+width, extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax2.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax2.set_title('Full phase error rate', fontsize=fontsize)\n",
    "ax2.set_xticks(ind)\n",
    "ax2.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax2.sharey(ax1) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind, interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind+width, extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax3.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax3.set_title('Log loss', fontsize=fontsize)\n",
    "ax3.set_xticks(ind)\n",
    "ax3.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax3.set_ylabel('logloss', fontsize=fontsize)\n",
    "plt.savefig(os.path.join(OUTPUTPATH, 'overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere  worm  vesicle  other\n",
       "0     0.0   1.0      0.0    0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data.y.iloc[test_indx,].mean()).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sphere     0.0\n",
       "worm       1.0\n",
       "vesicle    0.0\n",
       "other      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.iloc[test_indx,].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0   0.241  0.414   0.345    0.0   22.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2   0.938    0.0     0.0  0.062   16.0\n",
       "3     0.5  0.312   0.188    0.0   15.0\n",
       "4   0.857    0.0     0.0  0.143   14.0\n",
       "5     1.0    0.0     0.0    0.0   11.0\n",
       "6     1.0    0.0     0.0    0.0    8.0\n",
       "7   0.875    0.0     0.0  0.125    8.0\n",
       "8     1.0    0.0     0.0    0.0    7.0\n",
       "9   0.222  0.556   0.222    0.0    7.0\n",
       "10    nan    nan     nan    nan    7.0\n",
       "11    1.0    0.0     0.0    0.0    7.0\n",
       "12    1.0    0.0     0.0    0.0    6.0\n",
       "13  0.667    0.0     0.0  0.333    6.0\n",
       "14    1.0    0.0     0.0    0.0    6.0\n",
       "15    1.0    0.0     0.0    0.0    6.0\n",
       "16  0.167  0.667   0.167    0.0    5.0\n",
       "17    0.5  0.333   0.167    0.0    3.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    1.0    0.0     0.0    0.0    2.0\n",
       "20    0.0    0.0     0.0    1.0    2.0\n",
       "21    0.0    0.0     1.0    0.0    1.0\n",
       "22    0.0    0.0     1.0    0.0    1.0\n",
       "23    0.0    0.0     0.0    1.0    1.0\n",
       "24    0.0    1.0     0.0    0.0    1.0\n",
       "25    0.0    1.0     0.0    0.0    1.0\n",
       "26    0.0    1.0     0.0    0.0    1.0\n",
       "27    0.0    1.0     0.0    0.0    1.0\n",
       "28    0.0    1.0     0.0    0.0    1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrapolation hold out distribution\n",
    "\n",
    "from modules.experiments import GroupKFoldSpecial\n",
    "import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sphere':[], 'worm': [], 'vesicle': [], 'other':[], 'counts':[]})\n",
    "kf = GroupKFoldSpecial(len(set(data.comp_ids)), size=22)\n",
    "for train_indx, test_indx in kf.split(data.x1, data.y.replace(-1, 0), groups=data.comp_ids.array):\n",
    "    temp = pd.DataFrame(data.y.iloc[test_indx,].sum()/sum(data.y.iloc[test_indx,].sum())).T\n",
    "    temp['counts'] = int(len(test_indx))\n",
    "    df = pd.concat([df, temp], ignore_index=True)\n",
    "df = df.round(3)\n",
    "df = df.astype(str)\n",
    "df\n",
    "# df.sphere = df.apply(lambda x: round(x.sphere) if x.sphere == 0 or x.sphere == 1 else x.sphere, axis=1)\n",
    "# df.loc[1, 'sphere'] = 0.99\n",
    "# df\n",
    "# df.round(3)#(0.000000, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.241</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0     nan    nan     nan    nan    7.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2     1.0    0.0     0.0    0.0   11.0\n",
       "3     1.0    0.0     0.0    0.0    8.0\n",
       "4     1.0    0.0     0.0    0.0    7.0\n",
       "5     1.0    0.0     0.0    0.0    7.0\n",
       "6     1.0    0.0     0.0    0.0    6.0\n",
       "7     1.0    0.0     0.0    0.0    6.0\n",
       "8     1.0    0.0     0.0    0.0    6.0\n",
       "9     1.0    0.0     0.0    0.0    2.0\n",
       "10    0.0    1.0     0.0    0.0    1.0\n",
       "11    0.0    1.0     0.0    0.0    1.0\n",
       "12    0.0    1.0     0.0    0.0    1.0\n",
       "13    0.0    1.0     0.0    0.0    1.0\n",
       "14    0.0    1.0     0.0    0.0    1.0\n",
       "15    0.0    0.0     1.0    0.0    1.0\n",
       "16    0.0    0.0     1.0    0.0    1.0\n",
       "17    0.0    0.0     0.0    1.0    1.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    0.0    0.0     0.0    1.0    2.0\n",
       "20  0.241  0.414   0.345    0.0   22.0\n",
       "21  0.938    0.0     0.0  0.062   16.0\n",
       "22    0.5  0.312   0.188    0.0   15.0\n",
       "23  0.857    0.0     0.0  0.143   14.0\n",
       "24  0.875    0.0     0.0  0.125    8.0\n",
       "25  0.222  0.556   0.222    0.0    7.0\n",
       "26  0.667    0.0     0.0  0.333    6.0\n",
       "27  0.167  0.667   0.167    0.0    5.0\n",
       "28    0.5  0.333   0.167    0.0    3.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reindex([10, 1, 5, 6, 8, 11, 12, 14, 15, 19, 24, 25, 26, 27, 28, 21, 22, 23, 18, 20, 0, 2, 3, 4, 7, 9, 13, 16, 17]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corona_GMA, core_HEMA has no morphology"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
