{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data\n",
    "from common import OUTPUTPATH\n",
    "from modules.experiments import KFold, GroupKFoldSpecial\n",
    "STATE = np.random.RandomState(seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.207393</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.217610</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>0.578349</td>\n",
       "      <td>0.020932</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.136065</td>\n",
       "      <td>2.306448</td>\n",
       "      <td>0.058633</td>\n",
       "      <td>2.407374</td>\n",
       "      <td>0.404062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036958</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>0.036325</td>\n",
       "      <td>0.122728</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.278947</td>\n",
       "      <td>0.113918</td>\n",
       "      <td>0.657340</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>1.289729</td>\n",
       "      <td>0.474532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.063004</td>\n",
       "      <td>0.024034</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.074966</td>\n",
       "      <td>0.231985</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.937478</td>\n",
       "      <td>0.427617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.067566</td>\n",
       "      <td>0.028430</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>0.072806</td>\n",
       "      <td>0.245469</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>1.152941</td>\n",
       "      <td>0.806021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.207393                0.006881   \n",
       "GAM                     0.036958                0.003932   \n",
       "RuleFit                 0.005315                0.001077   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.217610               0.055729          0.578349   \n",
       "GAM                    0.091776               0.036325          0.122728   \n",
       "RuleFit                0.063004               0.024034          0.016484   \n",
       "RF                     0.067566               0.028430          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.020932         0.606667        0.136065   \n",
       "GAM             0.012920         0.278947        0.113918   \n",
       "RuleFit         0.003913         0.199298        0.074966   \n",
       "RF              0.000319         0.207807        0.072806   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.306448            0.058633            2.407374   \n",
       "GAM                 0.657340            0.052355            1.289729   \n",
       "RuleFit             0.231985            0.009525            0.937478   \n",
       "RF                  0.245469            0.003132            1.152941   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.404062  \n",
       "GAM               0.474532  \n",
       "RuleFit           0.427617  \n",
       "RF                0.806021  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'interpolation.csv'))\n",
    "interpolation = interpolation.set_index('Unnamed: 0')\n",
    "interpolation.index.name = None\n",
    "interpolation.index = ['LR', 'GAM', 'RuleFit', 'RF']\n",
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.207095</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.207776</td>\n",
       "      <td>0.175107</td>\n",
       "      <td>0.572041</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.591613</td>\n",
       "      <td>0.453976</td>\n",
       "      <td>2.290821</td>\n",
       "      <td>0.076780</td>\n",
       "      <td>2.878996</td>\n",
       "      <td>2.019150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.146785</td>\n",
       "      <td>0.181733</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.363659</td>\n",
       "      <td>0.420920</td>\n",
       "      <td>0.698547</td>\n",
       "      <td>0.048496</td>\n",
       "      <td>2.461090</td>\n",
       "      <td>3.041846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.106868</td>\n",
       "      <td>0.155031</td>\n",
       "      <td>0.014548</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.290568</td>\n",
       "      <td>0.406958</td>\n",
       "      <td>0.220223</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>2.313745</td>\n",
       "      <td>3.107373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.116324</td>\n",
       "      <td>0.180780</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.266713</td>\n",
       "      <td>0.389427</td>\n",
       "      <td>0.241264</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>1.589073</td>\n",
       "      <td>1.722327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.207095                0.008958   \n",
       "GAM                     0.040741                0.002913   \n",
       "RuleFit                 0.004830                0.001173   \n",
       "RF                      0.000294                0.000472   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.207776               0.175107          0.572041   \n",
       "GAM                    0.146785               0.181733          0.135459   \n",
       "RuleFit                0.106868               0.155031          0.014548   \n",
       "RF                     0.116324               0.180780          0.000588   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.028520         0.591613        0.453976   \n",
       "GAM             0.009794         0.363659        0.420920   \n",
       "RuleFit         0.003273         0.290568        0.406958   \n",
       "RF              0.000943         0.266713        0.389427   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.290821            0.076780            2.878996   \n",
       "GAM                 0.698547            0.048496            2.461090   \n",
       "RuleFit             0.220223            0.018055            2.313745   \n",
       "RF                  0.241264            0.003835            1.589073   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                2.019150  \n",
       "GAM               3.041846  \n",
       "RuleFit           3.107373  \n",
       "RF                1.722327  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'extrapolation.csv'))\n",
    "extrapolation = extrapolation.set_index('Unnamed: 0')\n",
    "extrapolation.index.name = None\n",
    "extrapolation.index = ['LR', 'GAM', 'RuleFit', 'RF']\n",
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_summary(metric, summ, num_reps=30, baseline=None, names=None, colors = list(mcolors.BASE_COLORS.keys())):\n",
    "    width = 0.35\n",
    "    ind = np.arange(len(summ))\n",
    "    plt.bar(ind-width/2, summ[f'mean_train_{metric}'], width=width, label='train', \n",
    "            yerr=summ[f'std_train_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    plt.bar(ind+width/2, summ[f'mean_test_{metric}'], width=width, label='test',\n",
    "            yerr=summ[f'std_test_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    if baseline:\n",
    "        for i in range(len(baseline)):\n",
    "            plt.axhline(y=baseline[i], color=colors[i], linestyle='-', label=names[i])\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(ind, summ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_loss(_train, _test):\n",
    "    \"\"\"This is calculate realistic full phase error\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if (test_uniq[i] == _select).all():\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)\n",
    "\n",
    "def get_hamming_loss(_train, _test, name):\n",
    "    \"\"\"This is calculate realistic hamming loss\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if test_uniq[i] == _select:\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full sphase error base line:\n",
      "Unreal inform error:0.5912162162162162 \n",
      "Uninform error:0.9375\n"
     ]
    }
   ],
   "source": [
    "# interpolation full phase\n",
    "uniq, cnts = np.unique(data.y.values, return_counts=True, axis=0)\n",
    "unreal_inter_full_info_error = 1 - max(cnts)/sum(cnts)\n",
    "inter_full_uninfo_error = 1 - (1/2)**4\n",
    "print('Interpolation full sphase error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_full_info_error, inter_full_uninfo_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2778716216216216 \n",
      "Uninform error:0.5\n"
     ]
    }
   ],
   "source": [
    "# interpolation hamming loss\n",
    "unreal_inter_hamming_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    error = 1 - max(cnt)/sum(cnt)\n",
    "    unreal_inter_hamming_loss.append(error)\n",
    "unreal_inter_hamming_loss = np.mean(unreal_inter_hamming_loss)\n",
    "inter_uninfo_hamming_loss = np.mean([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_hamming_loss, inter_uninfo_hamming_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Unreal inform error:0.5233630952380952\n"
     ]
    }
   ],
   "source": [
    "# extrapolation full phase loss\n",
    "unreal_extra_full_info_error = []\n",
    "cnt =0\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    unreal_extra_full_info_error.append(get_full_loss(train, test))\n",
    "unreal_extra_full_info_error = np.mean(unreal_extra_full_info_error)\n",
    "unreal_extra_full_info_error\n",
    "print('Extrapolation full sphase error base line:\\nUnreal inform error:{}'.format(unreal_extra_full_info_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2485863095238095\n"
     ]
    }
   ],
   "source": [
    "# extrapolation hamming loss\n",
    "unreal_extra_hamming_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "        unreal_extra_hamming_loss.append(get_hamming_loss(data.y.index.tolist(), test, each.name))\n",
    "unreal_extra_hamming_loss = np.mean(unreal_extra_hamming_loss)\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{}'.format(unreal_extra_hamming_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def neglikehood(p, y):\n",
    "    result = (-math.log2(p)*y - (1-y)*math.log2(1-p))\n",
    "    return (result)\n",
    "\n",
    "def get_full_logs(_train, _test):\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = [list(each) for each in train_uniq]\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = [list(each) for each in test_uniq]\n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)\n",
    "\n",
    "def get_hamming_logs(_train, _test, name):\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = train_uniq.tolist()\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = test_uniq.tolist()\n",
    "    \n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full phase logloss base line:\n",
      " Unreal Inform error:  2.539959783214337\n"
     ]
    }
   ],
   "source": [
    "# unrealistic interpolation\n",
    "uniq, cnt = np.unique(data.y.values, axis=0, return_counts=True)\n",
    "prob = cnt/sum(cnt)\n",
    "\n",
    "unreal_inter_full_info_loss = 0\n",
    "for each in cnt:\n",
    "    unreal_inter_full_info_loss += -each * math.log2(each/sum(cnt))\n",
    "unreal_inter_full_info_loss = unreal_inter_full_info_loss/sum(cnt)\n",
    "print(\"Interpolation full phase logloss base line:\\n Unreal Inform error: \", unreal_inter_full_info_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average morphology logloss base line:\n",
      "Unreal Inform logloss:0.7397973042571211\n"
     ]
    }
   ],
   "source": [
    "# interpolation average sphase informed\n",
    "unreal_inter_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_inter_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_inter_avg_info_loss = np.mean(unreal_inter_avg_info_loss)\n",
    "print('Interpolation average morphology logloss base line:\\nUnreal Inform logloss:{}'.format(unreal_inter_avg_info_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Real Inform logloss:2.6603067710306796 \n",
      "Uninform logloss:4.0\n"
     ]
    }
   ],
   "source": [
    "real_extra_full_info_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    real_extra_full_info_loss.append(get_full_logs(train, test))\n",
    "real_extra_full_info_loss = np.mean(real_extra_full_info_loss)\n",
    "\n",
    "extra_full_uninfo_loss = -math.log2(0.5**4)\n",
    "\n",
    "print('Extrapolation full sphase error base line:\\nReal Inform logloss:{} \\nUninform logloss:{}'.format(real_extra_full_info_loss, extra_full_uninfo_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation average sphase error base line:\n",
      "Real Inform logloss:0.7397973042571211\n"
     ]
    }
   ],
   "source": [
    "unreal_extra_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_extra_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_extra_avg_info_loss = np.mean(unreal_extra_avg_info_loss)\n",
    "print('Extrapolation average sphase error base line:\\nReal Inform logloss:{}'.format(unreal_extra_avg_info_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.207393</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.217610</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>0.578349</td>\n",
       "      <td>0.020932</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.136065</td>\n",
       "      <td>2.306448</td>\n",
       "      <td>0.058633</td>\n",
       "      <td>2.407374</td>\n",
       "      <td>0.404062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036958</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>0.091776</td>\n",
       "      <td>0.036325</td>\n",
       "      <td>0.122728</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.278947</td>\n",
       "      <td>0.113918</td>\n",
       "      <td>0.657340</td>\n",
       "      <td>0.052355</td>\n",
       "      <td>1.289729</td>\n",
       "      <td>0.474532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.063004</td>\n",
       "      <td>0.024034</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.074966</td>\n",
       "      <td>0.231985</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.937478</td>\n",
       "      <td>0.427617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.067566</td>\n",
       "      <td>0.028430</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.207807</td>\n",
       "      <td>0.072806</td>\n",
       "      <td>0.245469</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>1.152941</td>\n",
       "      <td>0.806021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.207393                0.006881   \n",
       "GAM                     0.036958                0.003932   \n",
       "RuleFit                 0.005315                0.001077   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.217610               0.055729          0.578349   \n",
       "GAM                    0.091776               0.036325          0.122728   \n",
       "RuleFit                0.063004               0.024034          0.016484   \n",
       "RF                     0.067566               0.028430          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.020932         0.606667        0.136065   \n",
       "GAM             0.012920         0.278947        0.113918   \n",
       "RuleFit         0.003913         0.199298        0.074966   \n",
       "RF              0.000319         0.207807        0.072806   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.306448            0.058633            2.407374   \n",
       "GAM                 0.657340            0.052355            1.289729   \n",
       "RuleFit             0.231985            0.009525            0.937478   \n",
       "RF                  0.245469            0.003132            1.152941   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.404062  \n",
       "GAM               0.474532  \n",
       "RuleFit           0.427617  \n",
       "RF                0.806021  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.207095</td>\n",
       "      <td>0.008958</td>\n",
       "      <td>0.207776</td>\n",
       "      <td>0.175107</td>\n",
       "      <td>0.572041</td>\n",
       "      <td>0.028520</td>\n",
       "      <td>0.591613</td>\n",
       "      <td>0.453976</td>\n",
       "      <td>2.290821</td>\n",
       "      <td>0.076780</td>\n",
       "      <td>2.878996</td>\n",
       "      <td>2.019150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.146785</td>\n",
       "      <td>0.181733</td>\n",
       "      <td>0.135459</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>0.363659</td>\n",
       "      <td>0.420920</td>\n",
       "      <td>0.698547</td>\n",
       "      <td>0.048496</td>\n",
       "      <td>2.461090</td>\n",
       "      <td>3.041846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.106868</td>\n",
       "      <td>0.155031</td>\n",
       "      <td>0.014548</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.290568</td>\n",
       "      <td>0.406958</td>\n",
       "      <td>0.220223</td>\n",
       "      <td>0.018055</td>\n",
       "      <td>2.313745</td>\n",
       "      <td>3.107373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.116324</td>\n",
       "      <td>0.180780</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.266713</td>\n",
       "      <td>0.389427</td>\n",
       "      <td>0.241264</td>\n",
       "      <td>0.003835</td>\n",
       "      <td>1.589073</td>\n",
       "      <td>1.722327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.207095                0.008958   \n",
       "GAM                     0.040741                0.002913   \n",
       "RuleFit                 0.004830                0.001173   \n",
       "RF                      0.000294                0.000472   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.207776               0.175107          0.572041   \n",
       "GAM                    0.146785               0.181733          0.135459   \n",
       "RuleFit                0.106868               0.155031          0.014548   \n",
       "RF                     0.116324               0.180780          0.000588   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.028520         0.591613        0.453976   \n",
       "GAM             0.009794         0.363659        0.420920   \n",
       "RuleFit         0.003273         0.290568        0.406958   \n",
       "RF              0.000943         0.266713        0.389427   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.290821            0.076780            2.878996   \n",
       "GAM                 0.698547            0.048496            2.461090   \n",
       "RuleFit             0.220223            0.018055            2.313745   \n",
       "RF                  0.241264            0.003835            1.589073   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                2.019150  \n",
       "GAM               3.041846  \n",
       "RuleFit           3.107373  \n",
       "RF                1.722327  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'c', 'm', 'y', 'b']\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, sharex=True, figsize=(20, 5))\n",
    "\n",
    "width = 0.25\n",
    "fontsize = 10\n",
    "ind = np.arange(len(interpolation))\n",
    "\n",
    "ax1.bar(ind-width, interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind, interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind+width, extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax1.set_title('Individual morphology error rate', fontsize=fontsize)\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax1.set_xticklabels(interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax1.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "ax1.sharey(ax2) ########### here to share the y axis\n",
    "\n",
    "ax2.bar(ind-width, interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind, interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind+width, extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax2.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax2.set_title('Full phase error rate', fontsize=fontsize)\n",
    "ax2.set_xticks(ind)\n",
    "ax2.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax2.sharey(ax1) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind, interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind+width, extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax3.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax3.set_title('Log loss', fontsize=fontsize)\n",
    "ax3.set_xticks(ind)\n",
    "ax3.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax3.set_ylabel('logloss', fontsize=fontsize)\n",
    "plt.savefig(os.path.join(OUTPUTPATH, 'latest_overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0   0.333  0.364   0.303    0.0   22.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2   0.938    0.0     0.0  0.062   16.0\n",
       "3     0.5  0.312   0.188    0.0   15.0\n",
       "4   0.857    0.0     0.0  0.143   14.0\n",
       "5     1.0    0.0     0.0    0.0   11.0\n",
       "6     1.0    0.0     0.0    0.0    8.0\n",
       "7   0.875    0.0     0.0  0.125    8.0\n",
       "8     1.0    0.0     0.0    0.0    7.0\n",
       "9   0.222  0.556   0.222    0.0    7.0\n",
       "10    nan    nan     nan    nan    7.0\n",
       "11    1.0    0.0     0.0    0.0    7.0\n",
       "12    1.0    0.0     0.0    0.0    6.0\n",
       "13  0.667    0.0     0.0  0.333    6.0\n",
       "14    1.0    0.0     0.0    0.0    6.0\n",
       "15    1.0    0.0     0.0    0.0    6.0\n",
       "16  0.167  0.667   0.167    0.0    5.0\n",
       "17    0.5  0.333   0.167    0.0    3.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    1.0    0.0     0.0    0.0    2.0\n",
       "20    0.0    0.0     0.0    1.0    2.0\n",
       "21    0.0    0.0     1.0    0.0    1.0\n",
       "22    0.0    0.0     1.0    0.0    1.0\n",
       "23    0.0    0.0     0.0    1.0    1.0\n",
       "24    0.0    1.0     0.0    0.0    1.0\n",
       "25    0.0    1.0     0.0    0.0    1.0\n",
       "26    0.0    1.0     0.0    0.0    1.0\n",
       "27    0.0    1.0     0.0    0.0    1.0\n",
       "28    0.0    1.0     0.0    0.0    1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrapolation hold out distribution\n",
    "\n",
    "from modules.experiments import GroupKFoldSpecial\n",
    "import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sphere':[], 'worm': [], 'vesicle': [], 'other':[], 'counts':[]})\n",
    "kf = GroupKFoldSpecial(len(set(data.comp_ids)), size=22)\n",
    "for train_indx, test_indx in kf.split(data.x1, data.y.replace(-1, 0), groups=data.comp_ids.array):\n",
    "    temp = pd.DataFrame(data.y.iloc[test_indx,].sum()/sum(data.y.iloc[test_indx,].sum())).T\n",
    "    temp['counts'] = int(len(test_indx))\n",
    "    df = pd.concat([df, temp], ignore_index=True)\n",
    "df = df.round(3)\n",
    "df = df.astype(str)\n",
    "df\n",
    "# df.sphere = df.apply(lambda x: round(x.sphere) if x.sphere == 0 or x.sphere == 1 else x.sphere, axis=1)\n",
    "# df.loc[1, 'sphere'] = 0.99\n",
    "# df\n",
    "# df.round(3)#(0.000000, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0   0.333  0.364   0.303    0.0   22.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2   0.938    0.0     0.0  0.062   16.0\n",
       "3     0.5  0.312   0.188    0.0   15.0\n",
       "4   0.857    0.0     0.0  0.143   14.0\n",
       "5     1.0    0.0     0.0    0.0   11.0\n",
       "6     1.0    0.0     0.0    0.0    8.0\n",
       "7   0.875    0.0     0.0  0.125    8.0\n",
       "8     1.0    0.0     0.0    0.0    7.0\n",
       "9   0.222  0.556   0.222    0.0    7.0\n",
       "10    nan    nan     nan    nan    7.0\n",
       "11    1.0    0.0     0.0    0.0    7.0\n",
       "12    1.0    0.0     0.0    0.0    6.0\n",
       "13  0.667    0.0     0.0  0.333    6.0\n",
       "14    1.0    0.0     0.0    0.0    6.0\n",
       "15    1.0    0.0     0.0    0.0    6.0\n",
       "16  0.167  0.667   0.167    0.0    5.0\n",
       "17    0.5  0.333   0.167    0.0    3.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    1.0    0.0     0.0    0.0    2.0\n",
       "20    0.0    0.0     0.0    1.0    2.0\n",
       "21    0.0    0.0     1.0    0.0    1.0\n",
       "22    0.0    0.0     1.0    0.0    1.0\n",
       "23    0.0    0.0     0.0    1.0    1.0\n",
       "24    0.0    1.0     0.0    0.0    1.0\n",
       "25    0.0    1.0     0.0    0.0    1.0\n",
       "26    0.0    1.0     0.0    0.0    1.0\n",
       "27    0.0    1.0     0.0    0.0    1.0\n",
       "28    0.0    1.0     0.0    0.0    1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extrapolation hold out distribution\n",
    "\n",
    "from modules.experiments import GroupKFoldSpecial\n",
    "import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sphere':[], 'worm': [], 'vesicle': [], 'other':[], 'counts':[]})\n",
    "kf = GroupKFoldSpecial(len(set(data.comp_ids)), size=22)\n",
    "for train_indx, test_indx in kf.split(data.x1, data.y.replace(-1, 0), groups=data.comp_ids.array):\n",
    "    temp = pd.DataFrame(data.y.iloc[test_indx,].sum()/sum(data.y.iloc[test_indx,].sum())).T\n",
    "    temp['counts'] = int(len(test_indx))\n",
    "    df = pd.concat([df, temp], ignore_index=True)\n",
    "df = df.round(3)\n",
    "df = df.astype(str)\n",
    "df\n",
    "# df.sphere = df.apply(lambda x: round(x.sphere) if x.sphere == 0 or x.sphere == 1 else x.sphere, axis=1)\n",
    "# df.loc[1, 'sphere'] = 0.99\n",
    "# df\n",
    "# df.round(3)#(0.000000, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sphere</th>\n",
       "      <th>worm</th>\n",
       "      <th>vesicle</th>\n",
       "      <th>other</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.222</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.167</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sphere   worm vesicle  other counts\n",
       "0     nan    nan     nan    nan    7.0\n",
       "1     1.0    0.0     0.0    0.0   16.0\n",
       "2     1.0    0.0     0.0    0.0   11.0\n",
       "3     1.0    0.0     0.0    0.0    8.0\n",
       "4     1.0    0.0     0.0    0.0    7.0\n",
       "5     1.0    0.0     0.0    0.0    7.0\n",
       "6     1.0    0.0     0.0    0.0    6.0\n",
       "7     1.0    0.0     0.0    0.0    6.0\n",
       "8     1.0    0.0     0.0    0.0    6.0\n",
       "9     1.0    0.0     0.0    0.0    2.0\n",
       "10    0.0    1.0     0.0    0.0    1.0\n",
       "11    0.0    1.0     0.0    0.0    1.0\n",
       "12    0.0    1.0     0.0    0.0    1.0\n",
       "13    0.0    1.0     0.0    0.0    1.0\n",
       "14    0.0    1.0     0.0    0.0    1.0\n",
       "15    0.0    0.0     1.0    0.0    1.0\n",
       "16    0.0    0.0     1.0    0.0    1.0\n",
       "17    0.0    0.0     0.0    1.0    1.0\n",
       "18    0.0    0.0     0.0    1.0    2.0\n",
       "19    0.0    0.0     0.0    1.0    2.0\n",
       "20  0.333  0.364   0.303    0.0   22.0\n",
       "21  0.938    0.0     0.0  0.062   16.0\n",
       "22    0.5  0.312   0.188    0.0   15.0\n",
       "23  0.857    0.0     0.0  0.143   14.0\n",
       "24  0.875    0.0     0.0  0.125    8.0\n",
       "25  0.222  0.556   0.222    0.0    7.0\n",
       "26  0.667    0.0     0.0  0.333    6.0\n",
       "27  0.167  0.667   0.167    0.0    5.0\n",
       "28    0.5  0.333   0.167    0.0    3.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reindex([10, 1, 5, 6, 8, 11, 12, 14, 15, 19, 24, 25, 26, 27, 28, 21, 22, 23, 18, 20, 0, 2, 3, 4, 7, 9, 13, 16, 17]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corona_GMA, core_HEMA has no morphology"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1304dd6425b372aa8285a68c11c3f25cfbbc0c3931e5f36be2c30c57a28c8b2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
