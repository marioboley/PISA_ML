{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data\n",
    "from common import OUTPUTPATH\n",
    "from modules.experiments import KFold, GroupKFoldSpecial\n",
    "STATE = np.random.RandomState(seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>2.359290</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263058</td>\n",
       "      <td>0.476161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit</th>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.207544</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.323129</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>1.047064</td>\n",
       "      <td>0.429802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                    0.202688                0.010509   \n",
       "GAM                   0.036842                0.003132   \n",
       "RuFit                 0.013761                0.002088   \n",
       "RF                    0.000015                0.000080   \n",
       "\n",
       "       mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                   0.215175               0.055519          0.556380   \n",
       "GAM                  0.088377               0.033908          0.122612   \n",
       "RuFit                0.066754               0.033378          0.044968   \n",
       "RF                   0.065461               0.026606          0.000058   \n",
       "\n",
       "       std_train_error  mean_test_error  std_test_error  mean_train_log loss  \\\n",
       "LR            0.043451         0.588246        0.146107             2.251545   \n",
       "GAM           0.010827         0.270439        0.105647             0.647140   \n",
       "RuFit         0.005734         0.207544        0.090273             0.323129   \n",
       "RF            0.000319         0.199298        0.067024             0.243237   \n",
       "\n",
       "       std_train_log loss  mean_test_log loss  std_test_log loss  \n",
       "LR               0.100844            2.359290           0.414337  \n",
       "GAM              0.038226            1.263058           0.476161  \n",
       "RuFit            0.010737            1.047064           0.429802  \n",
       "RF               0.003150            1.069053           0.685669  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'interpolation.csv'))\n",
    "interpolation = interpolation.set_index('Unnamed: 0')\n",
    "interpolation.index.name = None\n",
    "interpolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.204733</td>\n",
       "      <td>0.011155</td>\n",
       "      <td>0.182432</td>\n",
       "      <td>0.134576</td>\n",
       "      <td>0.567365</td>\n",
       "      <td>0.041419</td>\n",
       "      <td>0.616143</td>\n",
       "      <td>0.435963</td>\n",
       "      <td>2.277663</td>\n",
       "      <td>0.099304</td>\n",
       "      <td>2.644404</td>\n",
       "      <td>1.741969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.172878</td>\n",
       "      <td>0.133714</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.364181</td>\n",
       "      <td>0.423148</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.052019</td>\n",
       "      <td>2.506141</td>\n",
       "      <td>3.413426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit</th>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.108725</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.312950</td>\n",
       "      <td>0.391676</td>\n",
       "      <td>0.319397</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>1.939785</td>\n",
       "      <td>2.700429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.108071</td>\n",
       "      <td>0.178174</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>0.382492</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>1.564812</td>\n",
       "      <td>1.846939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                    0.204733                0.011155   \n",
       "GAM                   0.040134                0.003514   \n",
       "RuFit                 0.013480                0.001450   \n",
       "RF                    0.000305                0.000477   \n",
       "\n",
       "       mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                   0.182432               0.134576          0.567365   \n",
       "GAM                  0.139766               0.172878          0.133714   \n",
       "RuFit                0.108725               0.143921          0.045264   \n",
       "RF                   0.108071               0.178174          0.000609   \n",
       "\n",
       "       std_train_error  mean_test_error  std_test_error  mean_train_log loss  \\\n",
       "LR            0.041419         0.616143        0.435963             2.277663   \n",
       "GAM           0.011004         0.364181        0.423148             0.671861   \n",
       "RuFit         0.004241         0.312950        0.391676             0.319397   \n",
       "RF            0.000953         0.246090        0.382492             0.238811   \n",
       "\n",
       "       std_train_log loss  mean_test_log loss  std_test_log loss  \n",
       "LR               0.099304            2.644404           1.741969  \n",
       "GAM              0.052019            2.506141           3.413426  \n",
       "RuFit            0.007212            1.939785           2.700429  \n",
       "RF               0.003490            1.564812           1.846939  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'extrapolation.csv'))\n",
    "extrapolation = extrapolation.set_index('Unnamed: 0')\n",
    "extrapolation.index.name = None\n",
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_summary(metric, summ, num_reps=30, baseline=None, names=None, colors = list(mcolors.BASE_COLORS.keys())):\n",
    "    width = 0.35\n",
    "    ind = np.arange(len(summ))\n",
    "    plt.bar(ind-width/2, summ[f'mean_train_{metric}'], width=width, label='train', \n",
    "            yerr=summ[f'std_train_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    plt.bar(ind+width/2, summ[f'mean_test_{metric}'], width=width, label='test',\n",
    "            yerr=summ[f'std_test_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    if baseline:\n",
    "        for i in range(len(baseline)):\n",
    "            plt.axhline(y=baseline[i], color=colors[i], linestyle='-', label=names[i])\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(ind, summ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_loss(_train, _test):\n",
    "    \"\"\"This is calculate realistic full phase error\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if (test_uniq[i] == _select).all():\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)\n",
    "\n",
    "def get_hamming_loss(_train, _test, name):\n",
    "    \"\"\"This is calculate realistic hamming loss\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if test_uniq[i] == _select:\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full sphase error base line:\n",
      "Unreal inform error:0.5912162162162162 \n",
      "Uninform error:0.9375\n"
     ]
    }
   ],
   "source": [
    "# interpolation full phase\n",
    "uniq, cnts = np.unique(data.y.values, return_counts=True, axis=0)\n",
    "unreal_inter_full_info_error = 1 - max(cnts)/sum(cnts)\n",
    "inter_full_uninfo_error = 1 - (1/2)**4\n",
    "print('Interpolation full sphase error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_full_info_error, inter_full_uninfo_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.27956081081081074 \n",
      "Uninform error:0.5\n"
     ]
    }
   ],
   "source": [
    "# interpolation hamming loss\n",
    "unreal_inter_hamming_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    error = 1 - max(cnt)/sum(cnt)\n",
    "    unreal_inter_hamming_loss.append(error)\n",
    "unreal_inter_hamming_loss = np.mean(unreal_inter_hamming_loss)\n",
    "inter_uninfo_hamming_loss = np.mean([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_hamming_loss, inter_uninfo_hamming_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Unreal inform error:0.5233630952380952\n"
     ]
    }
   ],
   "source": [
    "# extrapolation full phase loss\n",
    "unreal_extra_full_info_error = []\n",
    "cnt =0\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    unreal_extra_full_info_error.append(get_full_loss(train, test))\n",
    "unreal_extra_full_info_error = np.mean(unreal_extra_full_info_error)\n",
    "unreal_extra_full_info_error\n",
    "print('Extrapolation full sphase error base line:\\nUnreal inform error:{}'.format(unreal_extra_full_info_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2485863095238095\n"
     ]
    }
   ],
   "source": [
    "# extrapolation hamming loss\n",
    "unreal_extra_hamming_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "        unreal_extra_hamming_loss.append(get_hamming_loss(data.y.index.tolist(), test, each.name))\n",
    "unreal_extra_hamming_loss = np.mean(unreal_extra_hamming_loss)\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{}'.format(unreal_extra_hamming_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def neglikehood(p, y):\n",
    "    result = (-math.log2(p)*y - (1-y)*math.log2(1-p))\n",
    "    return (result)\n",
    "\n",
    "def get_full_logs(_train, _test):\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = [list(each) for each in train_uniq]\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = [list(each) for each in test_uniq]\n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)\n",
    "\n",
    "def get_hamming_logs(_train, _test, name):\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = train_uniq.tolist()\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = test_uniq.tolist()\n",
    "    \n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full phase logloss base line:\n",
      " Unreal Inform error:  2.5225679165070907\n"
     ]
    }
   ],
   "source": [
    "# unrealistic interpolation\n",
    "uniq, cnt = np.unique(data.y.values, axis=0, return_counts=True)\n",
    "prob = cnt/sum(cnt)\n",
    "\n",
    "unreal_inter_full_info_loss = 0\n",
    "for each in cnt:\n",
    "    unreal_inter_full_info_loss += -each * math.log2(each/sum(cnt))\n",
    "unreal_inter_full_info_loss = unreal_inter_full_info_loss/sum(cnt)\n",
    "print(\"Interpolation full phase logloss base line:\\n Unreal Inform error: \", unreal_inter_full_info_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average morphology logloss base line:\n",
      "Unreal Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "# interpolation average sphase informed\n",
    "unreal_inter_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_inter_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_inter_avg_info_loss = np.mean(unreal_inter_avg_info_loss)\n",
    "print('Interpolation average morphology logloss base line:\\nUnreal Inform logloss:{}'.format(unreal_inter_avg_info_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Real Inform logloss:2.6554775150876018 \n",
      "Uninform logloss:4.0\n"
     ]
    }
   ],
   "source": [
    "real_extra_full_info_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    real_extra_full_info_loss.append(get_full_logs(train, test))\n",
    "real_extra_full_info_loss = np.mean(real_extra_full_info_loss)\n",
    "\n",
    "extra_full_uninfo_loss = -math.log2(0.5**4)\n",
    "\n",
    "print('Extrapolation full sphase error base line:\\nReal Inform logloss:{} \\nUninform logloss:{}'.format(real_extra_full_info_loss, extra_full_uninfo_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation average sphase error base line:\n",
      "Real Inform logloss:0.7401933665227551\n"
     ]
    }
   ],
   "source": [
    "unreal_extra_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_extra_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_extra_avg_info_loss = np.mean(unreal_extra_avg_info_loss)\n",
    "print('Extrapolation average sphase error base line:\\nReal Inform logloss:{}'.format(unreal_extra_avg_info_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>2.359290</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263058</td>\n",
       "      <td>0.476161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit</th>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.207544</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.323129</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>1.047064</td>\n",
       "      <td>0.429802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                    0.202688                0.010509   \n",
       "GAM                   0.036842                0.003132   \n",
       "RuFit                 0.013761                0.002088   \n",
       "RF                    0.000015                0.000080   \n",
       "\n",
       "       mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                   0.215175               0.055519          0.556380   \n",
       "GAM                  0.088377               0.033908          0.122612   \n",
       "RuFit                0.066754               0.033378          0.044968   \n",
       "RF                   0.065461               0.026606          0.000058   \n",
       "\n",
       "       std_train_error  mean_test_error  std_test_error  mean_train_log loss  \\\n",
       "LR            0.043451         0.588246        0.146107             2.251545   \n",
       "GAM           0.010827         0.270439        0.105647             0.647140   \n",
       "RuFit         0.005734         0.207544        0.090273             0.323129   \n",
       "RF            0.000319         0.199298        0.067024             0.243237   \n",
       "\n",
       "       std_train_log loss  mean_test_log loss  std_test_log loss  \n",
       "LR               0.100844            2.359290           0.414337  \n",
       "GAM              0.038226            1.263058           0.476161  \n",
       "RuFit            0.010737            1.047064           0.429802  \n",
       "RF               0.003150            1.069053           0.685669  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.204733</td>\n",
       "      <td>0.011155</td>\n",
       "      <td>0.182432</td>\n",
       "      <td>0.134576</td>\n",
       "      <td>0.567365</td>\n",
       "      <td>0.041419</td>\n",
       "      <td>0.616143</td>\n",
       "      <td>0.435963</td>\n",
       "      <td>2.277663</td>\n",
       "      <td>0.099304</td>\n",
       "      <td>2.644404</td>\n",
       "      <td>1.741969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.040134</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.172878</td>\n",
       "      <td>0.133714</td>\n",
       "      <td>0.011004</td>\n",
       "      <td>0.364181</td>\n",
       "      <td>0.423148</td>\n",
       "      <td>0.671861</td>\n",
       "      <td>0.052019</td>\n",
       "      <td>2.506141</td>\n",
       "      <td>3.413426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit</th>\n",
       "      <td>0.013480</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.108725</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.312950</td>\n",
       "      <td>0.391676</td>\n",
       "      <td>0.319397</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>1.939785</td>\n",
       "      <td>2.700429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.108071</td>\n",
       "      <td>0.178174</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.246090</td>\n",
       "      <td>0.382492</td>\n",
       "      <td>0.238811</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>1.564812</td>\n",
       "      <td>1.846939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                    0.204733                0.011155   \n",
       "GAM                   0.040134                0.003514   \n",
       "RuFit                 0.013480                0.001450   \n",
       "RF                    0.000305                0.000477   \n",
       "\n",
       "       mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                   0.182432               0.134576          0.567365   \n",
       "GAM                  0.139766               0.172878          0.133714   \n",
       "RuFit                0.108725               0.143921          0.045264   \n",
       "RF                   0.108071               0.178174          0.000609   \n",
       "\n",
       "       std_train_error  mean_test_error  std_test_error  mean_train_log loss  \\\n",
       "LR            0.041419         0.616143        0.435963             2.277663   \n",
       "GAM           0.011004         0.364181        0.423148             0.671861   \n",
       "RuFit         0.004241         0.312950        0.391676             0.319397   \n",
       "RF            0.000953         0.246090        0.382492             0.238811   \n",
       "\n",
       "       std_train_log loss  mean_test_log loss  std_test_log loss  \n",
       "LR               0.099304            2.644404           1.741969  \n",
       "GAM              0.052019            2.506141           3.413426  \n",
       "RuFit            0.007212            1.939785           2.700429  \n",
       "RF               0.003490            1.564812           1.846939  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'c', 'm', 'y', 'b']\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, sharex=True, figsize=(20, 5))\n",
    "\n",
    "width = 0.25\n",
    "fontsize = 10\n",
    "ind = np.arange(len(interpolation))\n",
    "\n",
    "ax1.bar(ind-width, interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind, interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind+width, extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax1.set_title('Individual morphology error rate', fontsize=fontsize)\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax1.set_xticklabels(interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax1.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "ax1.sharey(ax2) ########### here to share the y axis\n",
    "\n",
    "ax2.bar(ind-width, interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind, interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind+width, extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax2.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax2.set_title('Full phase error rate', fontsize=fontsize)\n",
    "ax2.set_xticks(ind)\n",
    "ax2.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax2.sharey(ax1) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind, interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind+width, extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax3.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax3.set_title('Log loss', fontsize=fontsize)\n",
    "ax3.set_xticks(ind)\n",
    "ax3.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax3.set_ylabel('logloss', fontsize=fontsize)\n",
    "plt.savefig(os.path.join(OUTPUTPATH, 'overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
