{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to find project root directory by chaning to parent directory\n",
      "all good\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.exists('common.py'):\n",
    "    print('trying to find project root directory by chaning to parent directory')\n",
    "    os.chdir('..')\n",
    "if os.path.exists('common.py'):\n",
    "    print('all good')\n",
    "else:\n",
    "    print('could not find project root directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data\n",
    "from common import OUTPUTPATH\n",
    "from modules.experiments import KFold, GroupKFoldSpecial\n",
    "STATE = np.random.RandomState(seed=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lr</th>\n",
       "      <td>0.183269</td>\n",
       "      <td>0.007191</td>\n",
       "      <td>0.201535</td>\n",
       "      <td>0.049730</td>\n",
       "      <td>0.474907</td>\n",
       "      <td>0.028398</td>\n",
       "      <td>0.513421</td>\n",
       "      <td>0.131641</td>\n",
       "      <td>2.073205</td>\n",
       "      <td>0.085173</td>\n",
       "      <td>2.159589</td>\n",
       "      <td>0.388495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.038438</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.043192</td>\n",
       "      <td>0.127604</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.264649</td>\n",
       "      <td>0.121913</td>\n",
       "      <td>0.631576</td>\n",
       "      <td>0.040820</td>\n",
       "      <td>1.278009</td>\n",
       "      <td>0.602032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit</th>\n",
       "      <td>0.010961</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>0.063092</td>\n",
       "      <td>0.037165</td>\n",
       "      <td>0.035087</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.182281</td>\n",
       "      <td>0.093626</td>\n",
       "      <td>0.267426</td>\n",
       "      <td>0.028055</td>\n",
       "      <td>0.989179</td>\n",
       "      <td>0.660852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.060439</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.192632</td>\n",
       "      <td>0.098014</td>\n",
       "      <td>0.237679</td>\n",
       "      <td>0.004404</td>\n",
       "      <td>1.038604</td>\n",
       "      <td>0.709325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean_train_hamming loss  std_train_hamming loss  \\\n",
       "Lr                    0.183269                0.007191   \n",
       "GAM                   0.038438                0.003724   \n",
       "RuFit                 0.010961                0.002383   \n",
       "RF                    0.000227                0.000390   \n",
       "\n",
       "       mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "Lr                   0.201535               0.049730          0.474907   \n",
       "GAM                  0.085000               0.043192          0.127604   \n",
       "RuFit                0.063092               0.037165          0.035087   \n",
       "RF                   0.060439               0.035364          0.000483   \n",
       "\n",
       "       std_train_error  mean_test_error  std_test_error  mean_train_log loss  \\\n",
       "Lr            0.028398         0.513421        0.131641             2.073205   \n",
       "GAM           0.012400         0.264649        0.121913             0.631576   \n",
       "RuFit         0.007635         0.182281        0.093626             0.267426   \n",
       "RF            0.000815         0.192632        0.098014             0.237679   \n",
       "\n",
       "       std_train_log loss  mean_test_log loss  std_test_log loss  \n",
       "Lr               0.085173            2.159589           0.388495  \n",
       "GAM              0.040820            1.278009           0.602032  \n",
       "RuFit            0.028055            0.989179           0.660852  \n",
       "RF               0.004404            1.038604           0.709325  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'no_assemble_interpolation.csv'))\n",
    "interpolation = interpolation.set_index('Unnamed: 0')\n",
    "interpolation.index.name = None\n",
    "interpolation # remove no assemble data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.010509</td>\n",
       "      <td>0.215175</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>0.100844</td>\n",
       "      <td>2.359290</td>\n",
       "      <td>0.414337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM</th>\n",
       "      <td>0.036842</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.088377</td>\n",
       "      <td>0.033908</td>\n",
       "      <td>0.122612</td>\n",
       "      <td>0.010827</td>\n",
       "      <td>0.270439</td>\n",
       "      <td>0.105647</td>\n",
       "      <td>0.647140</td>\n",
       "      <td>0.038226</td>\n",
       "      <td>1.263058</td>\n",
       "      <td>0.476161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuleFit</th>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.066754</td>\n",
       "      <td>0.033378</td>\n",
       "      <td>0.044968</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.207544</td>\n",
       "      <td>0.090273</td>\n",
       "      <td>0.323129</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>1.047064</td>\n",
       "      <td>0.429802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.026606</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.199298</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>1.069053</td>\n",
       "      <td>0.685669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean_train_hamming loss  std_train_hamming loss  \\\n",
       "LR                      0.202688                0.010509   \n",
       "GAM                     0.036842                0.003132   \n",
       "RuleFit                 0.013761                0.002088   \n",
       "RF                      0.000015                0.000080   \n",
       "\n",
       "         mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "LR                     0.215175               0.055519          0.556380   \n",
       "GAM                    0.088377               0.033908          0.122612   \n",
       "RuleFit                0.066754               0.033378          0.044968   \n",
       "RF                     0.065461               0.026606          0.000058   \n",
       "\n",
       "         std_train_error  mean_test_error  std_test_error  \\\n",
       "LR              0.043451         0.588246        0.146107   \n",
       "GAM             0.010827         0.270439        0.105647   \n",
       "RuleFit         0.005734         0.207544        0.090273   \n",
       "RF              0.000319         0.199298        0.067024   \n",
       "\n",
       "         mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "LR                  2.251545            0.100844            2.359290   \n",
       "GAM                 0.647140            0.038226            1.263058   \n",
       "RuleFit             0.323129            0.010737            1.047064   \n",
       "RF                  0.243237            0.003150            1.069053   \n",
       "\n",
       "         std_test_log loss  \n",
       "LR                0.414337  \n",
       "GAM               0.476161  \n",
       "RuleFit           0.429802  \n",
       "RF                0.685669  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_interpolation = pd.read_csv(os.path.join(OUTPUTPATH, 'interpolation.csv'))\n",
    "all_interpolation = all_interpolation.set_index('Unnamed: 0')\n",
    "all_interpolation.index.name = None\n",
    "all_interpolation # all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lr_pcc</th>\n",
       "      <td>0.183836</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>0.178331</td>\n",
       "      <td>0.479947</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.499150</td>\n",
       "      <td>0.454198</td>\n",
       "      <td>2.088094</td>\n",
       "      <td>0.096015</td>\n",
       "      <td>2.670335</td>\n",
       "      <td>1.991078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM_pcc</th>\n",
       "      <td>0.039552</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.134332</td>\n",
       "      <td>0.174609</td>\n",
       "      <td>0.130201</td>\n",
       "      <td>0.009977</td>\n",
       "      <td>0.327829</td>\n",
       "      <td>0.397306</td>\n",
       "      <td>0.625013</td>\n",
       "      <td>0.044157</td>\n",
       "      <td>2.668734</td>\n",
       "      <td>3.636575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit_pcc</th>\n",
       "      <td>0.010858</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.162652</td>\n",
       "      <td>0.202399</td>\n",
       "      <td>0.034836</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.353297</td>\n",
       "      <td>0.414054</td>\n",
       "      <td>0.263414</td>\n",
       "      <td>0.022423</td>\n",
       "      <td>2.316536</td>\n",
       "      <td>3.425546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_pcc</th>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.112563</td>\n",
       "      <td>0.184935</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.236691</td>\n",
       "      <td>0.367998</td>\n",
       "      <td>0.233382</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>1.504354</td>\n",
       "      <td>1.727774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_train_hamming loss  std_train_hamming loss  \\\n",
       "Lr_pcc                    0.183836                0.008626   \n",
       "GAM_pcc                   0.039552                0.002979   \n",
       "RuFit_pcc                 0.010858                0.002180   \n",
       "RF_pcc                    0.000094                0.000277   \n",
       "\n",
       "           mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "Lr_pcc                   0.183400               0.178331          0.479947   \n",
       "GAM_pcc                  0.134332               0.174609          0.130201   \n",
       "RuFit_pcc                0.162652               0.202399          0.034836   \n",
       "RF_pcc                   0.112563               0.184935          0.000188   \n",
       "\n",
       "           std_train_error  mean_test_error  std_test_error  \\\n",
       "Lr_pcc            0.029754         0.499150        0.454198   \n",
       "GAM_pcc           0.009977         0.327829        0.397306   \n",
       "RuFit_pcc         0.005773         0.353297        0.414054   \n",
       "RF_pcc            0.000553         0.236691        0.367998   \n",
       "\n",
       "           mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "Lr_pcc                2.088094            0.096015            2.670335   \n",
       "GAM_pcc               0.625013            0.044157            2.668734   \n",
       "RuFit_pcc             0.263414            0.022423            2.316536   \n",
       "RF_pcc                0.233382            0.003507            1.504354   \n",
       "\n",
       "           std_test_log loss  \n",
       "Lr_pcc              1.991078  \n",
       "GAM_pcc             3.636575  \n",
       "RuFit_pcc           3.425546  \n",
       "RF_pcc              1.727774  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'no_assemble_extrapolation.csv'))\n",
    "extrapolation = extrapolation.set_index('Unnamed: 0')\n",
    "extrapolation.index.name = None\n",
    "extrapolation # remove no assemble data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_hamming loss</th>\n",
       "      <th>std_train_hamming loss</th>\n",
       "      <th>mean_test_hamming loss</th>\n",
       "      <th>std_test_hamming loss</th>\n",
       "      <th>mean_train_error</th>\n",
       "      <th>std_train_error</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "      <th>mean_train_log loss</th>\n",
       "      <th>std_train_log loss</th>\n",
       "      <th>mean_test_log loss</th>\n",
       "      <th>std_test_log loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lr_pcc</th>\n",
       "      <td>0.203088</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>0.208413</td>\n",
       "      <td>0.167173</td>\n",
       "      <td>0.562837</td>\n",
       "      <td>0.039831</td>\n",
       "      <td>0.624005</td>\n",
       "      <td>0.437829</td>\n",
       "      <td>2.274070</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>2.761258</td>\n",
       "      <td>2.031499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAM_pcc</th>\n",
       "      <td>0.040066</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.146702</td>\n",
       "      <td>0.173823</td>\n",
       "      <td>0.133519</td>\n",
       "      <td>0.010856</td>\n",
       "      <td>0.376702</td>\n",
       "      <td>0.420958</td>\n",
       "      <td>0.671179</td>\n",
       "      <td>0.051213</td>\n",
       "      <td>2.545130</td>\n",
       "      <td>3.358488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RuFit_pcc</th>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>0.141384</td>\n",
       "      <td>0.184799</td>\n",
       "      <td>0.037851</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.337756</td>\n",
       "      <td>0.419485</td>\n",
       "      <td>0.273653</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>2.351826</td>\n",
       "      <td>3.136170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF_pcc</th>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.118843</td>\n",
       "      <td>0.184329</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.264250</td>\n",
       "      <td>0.388123</td>\n",
       "      <td>0.238704</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>1.640178</td>\n",
       "      <td>1.858515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean_train_hamming loss  std_train_hamming loss  \\\n",
       "Lr_pcc                    0.203088                0.011308   \n",
       "GAM_pcc                   0.040066                0.003470   \n",
       "RuFit_pcc                 0.011257                0.002541   \n",
       "RF_pcc                    0.000294                0.000472   \n",
       "\n",
       "           mean_test_hamming loss  std_test_hamming loss  mean_train_error  \\\n",
       "Lr_pcc                   0.208413               0.167173          0.562837   \n",
       "GAM_pcc                  0.146702               0.173823          0.133519   \n",
       "RuFit_pcc                0.141384               0.184799          0.037851   \n",
       "RF_pcc                   0.118843               0.184329          0.000588   \n",
       "\n",
       "           std_train_error  mean_test_error  std_test_error  \\\n",
       "Lr_pcc            0.039831         0.624005        0.437829   \n",
       "GAM_pcc           0.010856         0.376702        0.420958   \n",
       "RuFit_pcc         0.007831         0.337756        0.419485   \n",
       "RF_pcc            0.000943         0.264250        0.388123   \n",
       "\n",
       "           mean_train_log loss  std_train_log loss  mean_test_log loss  \\\n",
       "Lr_pcc                2.274070            0.098860            2.761258   \n",
       "GAM_pcc               0.671179            0.051213            2.545130   \n",
       "RuFit_pcc             0.273653            0.030820            2.351826   \n",
       "RF_pcc                0.238704            0.003475            1.640178   \n",
       "\n",
       "           std_test_log loss  \n",
       "Lr_pcc              2.031499  \n",
       "GAM_pcc             3.358488  \n",
       "RuFit_pcc           3.136170  \n",
       "RF_pcc              1.858515  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_extrapolation = pd.read_csv(os.path.join(OUTPUTPATH, 'extrapolation.csv'))\n",
    "all_extrapolation = all_extrapolation.set_index('Unnamed: 0')\n",
    "all_extrapolation.index.name = None\n",
    "all_extrapolation # contains all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_summary(metric, summ, num_reps=30, baseline=None, names=None, colors = list(mcolors.BASE_COLORS.keys())):\n",
    "    width = 0.35\n",
    "    ind = np.arange(len(summ))\n",
    "    plt.bar(ind-width/2, summ[f'mean_train_{metric}'], width=width, label='train', \n",
    "            yerr=summ[f'std_train_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    plt.bar(ind+width/2, summ[f'mean_test_{metric}'], width=width, label='test',\n",
    "            yerr=summ[f'std_test_{metric}']/num_reps**0.5, capsize=3.0)\n",
    "    if baseline:\n",
    "        for i in range(len(baseline)):\n",
    "            plt.axhline(y=baseline[i], color=colors[i], linestyle='-', label=names[i])\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.xticks(ind, summ.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_loss(_train, _test):\n",
    "    \"\"\"This is calculate realistic full phase error\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if (test_uniq[i] == _select).all():\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)\n",
    "\n",
    "def get_hamming_loss(_train, _test, name):\n",
    "    \"\"\"This is calculate realistic hamming loss\n",
    "    \"\"\"\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    _select = train_uniq[list(train_cnt).index(max(train_cnt))]\n",
    "\n",
    "    acc_cnt = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        if test_uniq[i] == _select:\n",
    "            acc_cnt += test_cnt[i]\n",
    "    return 1-acc_cnt/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full sphase error base line:\n",
      "Unreal inform error:0.5761821366024518 \n",
      "Uninform error:0.9375\n"
     ]
    }
   ],
   "source": [
    "# interpolation full phase\n",
    "uniq, cnts = np.unique(data.y.values, return_counts=True, axis=0)\n",
    "unreal_inter_full_info_error = 1 - max(cnts)/sum(cnts)\n",
    "inter_full_uninfo_error = 1 - (1/2)**4\n",
    "print('Interpolation full sphase error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_full_info_error, inter_full_uninfo_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is interpolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.2806479859894921 \n",
      "Uninform error:0.5\n"
     ]
    }
   ],
   "source": [
    "# interpolation hamming loss\n",
    "unreal_inter_hamming_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    error = 1 - max(cnt)/sum(cnt)\n",
    "    unreal_inter_hamming_loss.append(error)\n",
    "unreal_inter_hamming_loss = np.mean(unreal_inter_hamming_loss)\n",
    "inter_uninfo_hamming_loss = np.mean([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{} \\nUninform error:{}'.format(unreal_inter_hamming_loss, inter_uninfo_hamming_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed full phase loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Unreal inform error:0.5026709401709402\n"
     ]
    }
   ],
   "source": [
    "# extrapolation full phase loss\n",
    "unreal_extra_full_info_error = []\n",
    "cnt =0\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    unreal_extra_full_info_error.append(get_full_loss(train, test))\n",
    "unreal_extra_full_info_error = np.mean(unreal_extra_full_info_error)\n",
    "unreal_extra_full_info_error\n",
    "print('Extrapolation full sphase error base line:\\nUnreal inform error:{}'.format(unreal_extra_full_info_error))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is extrapolation unreal and uniformed hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average sphases error base line:\n",
      "Unreal inform error:0.24853395061728398\n"
     ]
    }
   ],
   "source": [
    "# extrapolation hamming loss\n",
    "unreal_extra_hamming_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "        unreal_extra_hamming_loss.append(get_hamming_loss(data.y.index.tolist(), test, each.name))\n",
    "unreal_extra_hamming_loss = np.mean(unreal_extra_hamming_loss)\n",
    "print('Interpolation average sphases error base line:\\nUnreal inform error:{}'.format(unreal_extra_hamming_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def neglikehood(p, y):\n",
    "    result = (-math.log2(p)*y - (1-y)*math.log2(1-p))\n",
    "    return (result)\n",
    "\n",
    "def get_full_logs(_train, _test):\n",
    "    train_uniq, train_cnt = np.unique(data.y[data.y.index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = [list(each) for each in train_uniq]\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[data.y.index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = [list(each) for each in test_uniq]\n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)\n",
    "\n",
    "def get_hamming_logs(_train, _test, name):\n",
    "    train_uniq, train_cnt = np.unique(data.y[name][data.y[name].index.isin(_train)].values, return_counts=True, axis=0)\n",
    "    train_uniq = train_uniq.tolist()\n",
    "    train_prob = train_cnt/sum(train_cnt)\n",
    "    \n",
    "    test_uniq, test_cnt = np.unique(data.y[name][data.y[name].index.isin(_test)], return_counts=True, axis=0)\n",
    "    test_uniq = test_uniq.tolist()\n",
    "    \n",
    "    logs = 0\n",
    "    for i in range(len(test_uniq)):\n",
    "        try:\n",
    "            indx = train_uniq.index(test_uniq[i])\n",
    "            logs += -1* test_cnt[i] * math.log2(train_prob[indx])\n",
    "        except:\n",
    "            pass\n",
    "    return logs/sum(test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation full phase logloss base line:\n",
      " Unreal Inform error:  2.3860728197730245\n"
     ]
    }
   ],
   "source": [
    "# unrealistic interpolation\n",
    "uniq, cnt = np.unique(data.y.values, axis=0, return_counts=True)\n",
    "prob = cnt/sum(cnt)\n",
    "\n",
    "unreal_inter_full_info_loss = 0\n",
    "for each in cnt:\n",
    "    unreal_inter_full_info_loss += -each * math.log2(each/sum(cnt))\n",
    "unreal_inter_full_info_loss = unreal_inter_full_info_loss/sum(cnt)\n",
    "print(\"Interpolation full phase logloss base line:\\n Unreal Inform error: \", unreal_inter_full_info_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation average morphology logloss base line:\n",
      "Unreal Inform logloss:0.7462898274008147\n"
     ]
    }
   ],
   "source": [
    "# interpolation average sphase informed\n",
    "unreal_inter_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_inter_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_inter_avg_info_loss = np.mean(unreal_inter_avg_info_loss)\n",
    "print('Interpolation average morphology logloss base line:\\nUnreal Inform logloss:{}'.format(unreal_inter_avg_info_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation full sphase error base line:\n",
      "Real Inform logloss:2.4879044454190025 \n",
      "Uninform logloss:4.0\n"
     ]
    }
   ],
   "source": [
    "real_extra_full_info_loss = []\n",
    "for train, test in GroupKFoldSpecial(len(set(data.comp_ids)), size=20).split(data.x1, data.y, data.comp_ids):\n",
    "    real_extra_full_info_loss.append(get_full_logs(train, test))\n",
    "real_extra_full_info_loss = np.mean(real_extra_full_info_loss)\n",
    "\n",
    "extra_full_uninfo_loss = -math.log2(0.5**4)\n",
    "\n",
    "print('Extrapolation full sphase error base line:\\nReal Inform logloss:{} \\nUninform logloss:{}'.format(real_extra_full_info_loss, extra_full_uninfo_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrapolation average sphase error base line:\n",
      "Real Inform logloss:0.7462898274008147\n"
     ]
    }
   ],
   "source": [
    "unreal_extra_avg_info_loss = []\n",
    "for each in [data.sphere, data.vesicle, data.worm, data.other]:\n",
    "    uniq, cnt = np.unique(each, return_counts=True)\n",
    "    prob = max(cnt)/sum(cnt) if uniq[list(cnt).index(max(cnt))] else 1- max(cnt)/sum(cnt)\n",
    "    \n",
    "    for indx in range(len(each)):\n",
    "        unreal_extra_avg_info_loss.append(neglikehood(prob, each[indx]))\n",
    "unreal_extra_avg_info_loss = np.mean(unreal_extra_avg_info_loss)\n",
    "print('Extrapolation average sphase error base line:\\nReal Inform logloss:{}'.format(unreal_extra_avg_info_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bf42da003492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0max3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = ['r', 'c', 'm', 'y', 'b']\n",
    "fig, ((ax1, ax11), (ax2, ax22), (ax3, ax33)) = plt.subplots(2, 3, sharey=False, sharex=True, figsize=(20, 5))\n",
    "\n",
    "width = 0.25\n",
    "fontsize = 10\n",
    "ind = np.arange(len(interpolation))\n",
    "\n",
    "ax1.bar(ind-width, interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind, interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax1.bar(ind+width, extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax1.set_title('Individual morphology error rate (Remove no_assembly)', fontsize=fontsize)\n",
    "ax1.set_xticks(ind)\n",
    "ax1.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax1.set_xticklabels(interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax1.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax1.legend(fontsize=fontsize)\n",
    "ax1.sharey(ax2) ########### here to share the y axis\n",
    "\n",
    "ax2.bar(ind-width, interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind, interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax2.bar(ind+width, extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax2.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax2.set_title('Full phase error rate (Remove no_assembly)', fontsize=fontsize)\n",
    "ax2.set_xticks(ind)\n",
    "ax2.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax2.sharey(ax1) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind, interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax3.bar(ind+width, extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax3.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax3.set_title('Log loss (Remove no_assembly)', fontsize=fontsize)\n",
    "ax3.set_xticks(ind)\n",
    "ax3.set_xticklabels(extrapolation.index, fontsize=fontsize)\n",
    "ax3.set_ylabel('logloss', fontsize=fontsize)\n",
    "# plt.savefig(os.path.join(OUTPUTPATH, 'no_assemble_overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "ax11.bar(ind-width, all_interpolation[f'mean_train_hamming loss'], width=width, label='training', \n",
    "        yerr=all_interpolation[f'std_train_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax11.bar(ind, all_interpolation[f'mean_test_hamming loss'], width=width, label='interpolation',\n",
    "        yerr=all_interpolation[f'std_test_hamming loss']/30**0.5, capsize=3.0)\n",
    "ax11.bar(ind+width, all_extrapolation[f'mean_test_hamming loss'], width=width, label='extrapolation',\n",
    "        yerr=all_extrapolation[f'std_test_hamming loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "ax11.set_title('Individual morphology error rate (all data)', fontsize=fontsize)\n",
    "ax11.set_xticks(ind)\n",
    "ax11.set_ylabel('Error rate', fontsize=fontsize)\n",
    "ax11.set_xticklabels(all_interpolation.index, fontsize=fontsize)\n",
    "\n",
    "# add base line\n",
    "avg_error_base = [inter_uninfo_hamming_loss, unreal_inter_hamming_loss]\n",
    "avg_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(avg_error_base)):\n",
    "    ax11.axhline(y=avg_error_base[i], color=colors[i], linestyle='-', label=avg_error_name[i])\n",
    "\n",
    "ax11.legend(fontsize=fontsize)\n",
    "ax11.sharey(ax22) ########### here to share the y axis\n",
    "\n",
    "ax22.bar(ind-width, all_interpolation[f'mean_train_error'], width=width, label='training', \n",
    "        yerr=all_interpolation[f'std_train_error']/30**0.5, capsize=3.0)\n",
    "ax22.bar(ind, all_interpolation[f'mean_test_error'], width=width, label='interpolation',\n",
    "        yerr=all_interpolation[f'std_test_error']/30**0.5, capsize=3.0)\n",
    "ax22.bar(ind+width, all_extrapolation[f'mean_test_error'], width=width, label='extrapolation',\n",
    "        yerr=all_extrapolation[f'std_test_error']/28**0.5, capsize=3.0)\n",
    "\n",
    "# add base line\n",
    "inter_error_base = [inter_full_uninfo_error, unreal_inter_full_info_error]\n",
    "inter_error_name = ['uninformed', 'informed']\n",
    "for i in range(len(inter_error_base)):\n",
    "    ax22.axhline(y=inter_error_base[i], color=colors[i], linestyle='-', label=inter_error_name[i])\n",
    "\n",
    "ax22.set_title('Full phase error rate (all data)', fontsize=fontsize)\n",
    "ax22.set_xticks(ind)\n",
    "ax22.set_xticklabels(all_extrapolation.index, fontsize=fontsize)\n",
    "ax22.sharey(ax11) ########### here to share the y axis\n",
    "\n",
    "\n",
    "ax33.bar(ind-width, all_interpolation[f'mean_train_log loss'], width=width, label='training', \n",
    "        yerr=all_interpolation[f'std_train_log loss']/30**0.5, capsize=3.0)\n",
    "ax33.bar(ind, all_interpolation[f'mean_test_log loss'], width=width, label='interpolation',\n",
    "        yerr=all_interpolation[f'std_test_log loss']/30**0.5, capsize=3.0)\n",
    "ax33.bar(ind+width, all_extrapolation[f'mean_test_log loss'], width=width, label='extrapolation',\n",
    "        yerr=all_extrapolation[f'std_test_log loss']/28**0.5, capsize=3.0)\n",
    "\n",
    "full_loss_base = [4, unreal_inter_full_info_loss]\n",
    "full_loss_name = ['uninformed', 'informed']\n",
    "for i in range(len(full_loss_base)):\n",
    "    ax33.axhline(y=full_loss_base[i], color=colors[i], linestyle='-', label=full_loss_name[i])\n",
    "\n",
    "ax33.set_title('Log loss (all data)', fontsize=fontsize)\n",
    "ax33.set_xticks(ind)\n",
    "ax33.set_xticklabels(all_extrapolation.index, fontsize=fontsize)\n",
    "ax33.set_ylabel('logloss', fontsize=fontsize)\n",
    "# plt.savefig(os.path.join(OUTPUTPATH, 'no_assemble_overall_performance.pdf'), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
